{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba08ac",
   "metadata": {},
   "source": [
    "## **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pytesseract\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "from openpyxl.utils import get_column_letter  # Add this line\n",
    "from commonfunctions import *\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' # (Windows Example)\n",
    "\n",
    "\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab16d",
   "metadata": {},
   "source": [
    "## **Main Functions Overview**\n",
    "\n",
    "- **Image Alignment**\n",
    "  - Detects SIFT keypoints and descriptors in the input and reference images.\n",
    "  - Matches them using the ratio test.\n",
    "  - Uses RANSAC to estimate a homography.\n",
    "  - Applies the homography to warp the input image so it lines up with the reference.\n",
    "  - Returns the aligned image (or the original if not enough matches are found).\n",
    "\n",
    "- **Extract Details**\n",
    "  - Uses (x, y, w, h) coordinates to crop the aligned card into:\n",
    "    - The name region\n",
    "    - The code (ID) region\n",
    "  - Returns these sub-images for downstream OCR or digit processing.\n",
    "\n",
    "- **Save Student Name**\n",
    "  - Ensures the output folder exists.\n",
    "  - Writes the cropped name image to disk with a filename that includes the student ID.\n",
    "  - Creates a persistent record usable for manual review or OCR.\n",
    "\n",
    "- **Split and Save Digits**\n",
    "  - Converts the code region to grayscale and applies Otsu thresholding.\n",
    "  - Finds contours and filters out small noise.\n",
    "  - Selects the largest seven contours (by area) and sorts them left-to-right.\n",
    "  - Saves each detected digit crop into a per-student folder as individual image files.\n",
    "\n",
    "- **save_split_digits**\n",
    "  - Takes a list of digit images for a student.\n",
    "  - Ensures a folder exists for each student (named by their ID).\n",
    "  - Saves each digit image as `digit_0.jpg`, `digit_1.jpg`, ..., `digit_6.jpg` inside the student’s folder.\n",
    "  - Used for batch saving when all digit crops are already extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945bafe",
   "metadata": {},
   "source": [
    "## **Noise Detection and Treatment**\n",
    "- **Impulsive Noise (Median Filter)**\n",
    "- **Random Noise (Gaussian Filter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_random_noise(img, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Detects if an image has random (Gaussian) noise.\n",
    "    If noise is above threshold, applies Gaussian blurring.\n",
    "    Returns the (possibly filtered) image and a boolean indicating if noise was detected and treated image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Estimate noise using a simple method: standard deviation of pixel intensities\n",
    "    stddev = np.std(img)\n",
    "    normalized_stddev = stddev / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    if normalized_stddev < threshold:\n",
    "        return img, False  # No significant noise, return original\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred_img = cv2.GaussianBlur(img, (7, 7), 0)\n",
    "    #show_images([img,blurred_img], [\"Original Image\", \"After Gaussian Blur\"])\n",
    "    return blurred_img, True\n",
    "\n",
    "# gray_img = cv2.imread('ykismail_College-ID-Scanner_main_images/ID10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# noisy_img = random_noise(gray_img, mode='gaussian', mean=0.5)\n",
    "# noisy_img = (noisy_img * 255).astype('uint8')  # Convert back to uint8 for OpenCV\n",
    "\n",
    "# TreatedImg, noise_detected = is_random_noise(noisy_img)\n",
    "# if noise_detected:\n",
    "#     print(\"Random noise detected and treated!\")\n",
    "# else:\n",
    "#     print(\"No random noise detected.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53737cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_impulsive_noise(img, threshold=0.1, black_range=(0, 9), white_range=(246, 255)):\n",
    "    \"\"\"\n",
    "    Detects if an image has impulsive (salt-and-pepper) noise.\n",
    "    If noise is above threshold, applies median filtering with adaptive kernel size.\n",
    "    Returns the (possibly filtered) image and a boolean indicating if noise was detected an treted image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    total_pixels = img.size\n",
    "    num_black = np.sum((img >= black_range[0]) & (img <= black_range[1]))\n",
    "    num_white = np.sum((img >= white_range[0]) & (img <= white_range[1]))\n",
    "    prop = (num_black + num_white) / total_pixels\n",
    "\n",
    "    if prop < threshold:\n",
    "        return img, False  # No significant noise, return original\n",
    "    # Determine kernel size based on noise severity\n",
    "    k = int(3 + prop * 10)\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    k = min(max(k, 3), 9)\n",
    "    filtered_img = cv2.medianBlur(img, k)\n",
    "    #show_images([img,filtered_img], [\"Original Image\", \"After median filter\"])\n",
    "    return filtered_img, True\n",
    "\n",
    "# gray_img = cv2.imread('ykismail_College-ID-Scanner_main_images/ID10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# noisy_img = random_noise(gray_img, mode='s&p', amount=0.6)\n",
    "# noisy_img = (noisy_img * 255).astype('uint8')  # Convert back to uint8 for OpenCV\n",
    "\n",
    "# TreatedImg, noise_detected = is_impulsive_noise(noisy_img)\n",
    "# if noise_detected:\n",
    "#     print(\"Impulsive noise detected and treated!\")\n",
    "# else:\n",
    "#     print(\"No impulsive noise detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images_sift(img_to_align, reference_path):\n",
    "    img1 = img_to_align\n",
    "    img2 = cv2.imread(reference_path)      # Train Image (The perfect template)\n",
    "    \n",
    "    # --- FIX: Check if img1 is already grayscale ---\n",
    "    if len(img1.shape) == 3:\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray1 = img1 # Already grayscale\n",
    "\n",
    "    # Ref image is loaded from disk, usually BGR, but good to check\n",
    "    if len(img2.shape) == 3:\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray2 = img2\n",
    "\n",
    "    sift = cv2.SIFT_create() \n",
    "    \n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        h, w = img2.shape[:2]\n",
    "        # Warp the original img1 (whether gray or color)\n",
    "        aligned_img = cv2.warpPerspective(img1, M, (w, h))\n",
    "\n",
    "        return aligned_img\n",
    "    \n",
    "    else:\n",
    "        print(f\"Not enough matches found: {len(good_matches)}/10\")\n",
    "        return img1\n",
    "    \n",
    "\n",
    "\n",
    "def extract_details(aligned_image):\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    \n",
    "    name_contour = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_contour = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    \n",
    "    return name_contour, code_contour\n",
    "\n",
    "\n",
    "def save_student_name(student_id, name_img, output_folder=\"extracted_names\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    # Construct filename: extracted_names/ID1_name.jpg\n",
    "    filename = f\"{output_folder}/{student_id}_name.jpg\"\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(filename, name_img)\n",
    "    \n",
    "\n",
    "def split_and_save_digits(student_id, code_roi, output_folder=\"extracted_digits\"):\n",
    "    save_path = f\"{output_folder}/ID{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    gray = cv2.cvtColor(code_roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # A. Collect all valid candidates\n",
    "    candidates = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        # Filter tiny noise\n",
    "        if h > 15 and w > 5:\n",
    "            candidates.append((x, y, w, h, area))\n",
    "            \n",
    "    # B. CRITICAL: Select exactly the top 7 by AREA (Size)\n",
    "    # This removes small specks or the colon \":\" if it was caught\n",
    "    candidates = sorted(candidates, key=lambda c: c[4], reverse=True) # Sort largest first\n",
    "    final_digits = candidates[:7] # Take top 7\n",
    "    \n",
    "    # C. Sort the final 7 by X-COORDINATE (Left -> Right)\n",
    "    # This puts them back in the correct reading order (1, 2, 3...)\n",
    "    final_digits = sorted(final_digits, key=lambda c: c[0])\n",
    "        \n",
    "    # D. Save\n",
    "    for index, (x, y, w, h, area) in enumerate(final_digits):\n",
    "        digit_img = code_roi[y:y+h, x:x+w]\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)\n",
    "\n",
    "      \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_name_and_digits(aligned_image):\n",
    "    \"\"\"\n",
    "    Input: An aligned ID card image.\n",
    "    Output: \n",
    "      - name_roi: The image of the extracted name.\n",
    "      - code_digits: A list of images for the code digits.\n",
    "      - daf3_digits: A list of images for the daf3 digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Hardcoded Coordinates ---\n",
    "    # x, y, w, h\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    daf3_coords = (350, 500, 620, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    dx, dy, dw, dh = daf3_coords\n",
    "    \n",
    "    # Extract ROIs\n",
    "    name_img = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_roi = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    daf3_img = aligned_image[dy:dy+dh, dx:dx+dw]\n",
    "    \n",
    "    # --- Helper Function to Process Any ROI ---\n",
    "    def process_roi_digits(roi_img, digit_limit):\n",
    "        \"\"\"\n",
    "        Applies grayscale, thresholding, contour detection, \n",
    "        splitting of merged digits, and sorting.\n",
    "        \"\"\"\n",
    "        #gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if len(roi_img.shape) == 3:\n",
    "            gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = roi_img\n",
    "        # --- FIX ENDS HERE --\n",
    "        \n",
    "        # Binary Inverse + Otsu\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        candidates = []\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            area = w * h\n",
    "            \n",
    "            # Filter tiny noise \n",
    "            if h > 15 and w > 5:\n",
    "                \n",
    "                # --- CHECK FOR MERGED DIGITS ---\n",
    "                # If width > 0.8 * height, likely two digits stuck together\n",
    "                if w > 0.8 * h: \n",
    "                    half_w = w // 2\n",
    "                    # Digit 1 (Left half)\n",
    "                    candidates.append((x, y, half_w, h, half_w * h))\n",
    "                    # Digit 2 (Right half)\n",
    "                    candidates.append((x + half_w, y, half_w, h, half_w * h))\n",
    "                else:\n",
    "                    # Normal single digit\n",
    "                    candidates.append((x, y, w, h, area))\n",
    "        \n",
    "        # 1. Sort by Area Descending (Keep only the largest objects to remove noise)\n",
    "        candidates = sorted(candidates, key=lambda c: c[4], reverse=True)[:digit_limit]\n",
    "        \n",
    "        # 2. Sort by X-coordinate Ascending (Order them Left -> Right)\n",
    "        final_candidates = sorted(candidates, key=lambda c: c[0])\n",
    "        \n",
    "        # Crop the actual images\n",
    "        cropped_digits = []\n",
    "        for (x, y, w, h, area) in final_candidates:\n",
    "            digit_crop = roi_img[y:y+h, x:x+w]\n",
    "            cropped_digits.append(digit_crop)\n",
    "            \n",
    "        return cropped_digits\n",
    "\n",
    "    # --- 2. Process Regions ---\n",
    "    \n",
    "    # Detect Code (Limit 7 digits)\n",
    "    code_digits = process_roi_digits(code_roi, digit_limit=7)\n",
    "    \n",
    "    # Detect Daf3 (Limit 14 digits)\n",
    "    daf3_digits = process_roi_digits(daf3_img, digit_limit=14)\n",
    "\n",
    "    return name_img, code_digits, daf3_digits\n",
    "\n",
    "def save_split_digits(student_id, digit_imgs, output_folder=\"extracted_digits\"):\n",
    "    \"\"\"\n",
    "    Saves a list of digit images for a student in the same way as split_and_save_digits.\n",
    "    Each digit is saved as digit_0.jpg, digit_1.jpg, ..., digit_6.jpg in a folder per student.\n",
    "    \"\"\"\n",
    "    save_path = f\"{output_folder}/{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for index, digit_img in enumerate(digit_imgs):\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f17014",
   "metadata": {},
   "source": [
    "## **SVM English Number Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_dataset = r\"train_digits\"  # Training set\n",
    "\n",
    "def train_SVM_robust():\n",
    "    # 1. Map your specific filename prefixes to actual digits\n",
    "    label_map = {\n",
    "        'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', \n",
    "        'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '9'\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    img_filenames = os.listdir(path_to_train_dataset)\n",
    "    print(f\"Loading {len(img_filenames)} training images...\")\n",
    "\n",
    "    for fn in img_filenames:\n",
    "        if not fn.lower().endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "\n",
    "        # Get the first letter (a, b, c...)\n",
    "        prefix = fn[0].lower()\n",
    "        if prefix in label_map:\n",
    "            labels.append(label_map[prefix])\n",
    "            \n",
    "            path = os.path.join(path_to_train_dataset, fn)\n",
    "            img = cv2.imread(path)\n",
    "            \n",
    "            # Extract HOG features (ensure preprocessing matches)\n",
    "            features.append(extract_hog_features(img))\n",
    "    \n",
    "    # 2. Create a Pipeline: Scale Features -> Train SVM\n",
    "    # Scaling is CRITICAL for HOG-based SVMs\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', LinearSVC(random_state=42, max_iter=5000, dual=False))\n",
    "    ])\n",
    "    \n",
    "    # 3. Train/Test Split for internal validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(f\"Training Complete. Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    # Ensure grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # APPLY IDENTICAL PREPROCESSING TO TRAINING AND TEST DATA\n",
    "    # This turns both sets into \"binary masks\" to ignore lighting/shadows\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32)) # target_img_size\n",
    "    \n",
    "    win_size = (32, 32)\n",
    "    cell_size = (8, 8)  # Slightly larger cells help ignore \"noise/shadows\"\n",
    "    block_size = (16, 16)\n",
    "    block_stride = (8, 8)\n",
    "    nbins = 9\n",
    "    \n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fbfe0",
   "metadata": {},
   "source": [
    "# **Tesseract Arabic OCR**\n",
    "\n",
    "## **Current Situation**\n",
    "\n",
    "The project uses Tesseract OCR to extract Arabic names from scanned images. Initially, the extraction pipeline achieved only a **70% success rate**. This meant that about 30% of the images failed to yield any valid Arabic text, even though the images were visually clear and contained readable names.\n",
    "\n",
    "## **Why Was the Success Rate Only 70%?**\n",
    "\n",
    "- **Overprocessing:** The original code applied several preprocessing steps (scaling, thresholding, blurring, etc.) before running OCR. While these steps can help with noisy or low-contrast images, they often **destroy clean, high-contrast text**—especially for Arabic, where fine details matter.\n",
    "- **Order of Operations:** The pipeline tried processed versions first, so if the original image was already optimal, it was never used for OCR.\n",
    "- **PSM/OEM Settings:** The code tried a limited set of Tesseract Page Segmentation Modes (PSM) and OCR Engine Modes (OEM), which may not have been optimal for all images.\n",
    "- **Text Cleaning:** The cleaning function was aggressive, but if Tesseract output was empty or too short, the result was discarded.\n",
    "\n",
    "## **What Was Changed to Achieve 100% Success**\n",
    "\n",
    "1. **Prioritize the Original Image:**  \n",
    "   The new code always tries the original, unprocessed grayscale image first, with several PSM settings. This ensures that clean images are not degraded by unnecessary processing.\n",
    "\n",
    "2. **Expanded Preprocessing (But Only If Needed):**  \n",
    "   Only if the original image fails, the code tries padded and scaled versions, but never applies destructive thresholding or blurring unless absolutely necessary.\n",
    "\n",
    "3. **Multiple PSM and OEM Combinations:**  \n",
    "   For each image variant, the code tries several PSM (6, 7, 3, 13) and both OEM (3, 1) settings, maximizing the chance that Tesseract will interpret the layout correctly.\n",
    "\n",
    "4. **Result Selection:**  \n",
    "   All non-empty results are collected, and the **longest valid extraction** is chosen, which is usually the correct full name.\n",
    "\n",
    "5. **Diagnostics:**  \n",
    "   Additional debug and diagnostic code was used to confirm that the original image, with minimal processing, consistently yields the best results for this dataset.\n",
    "\n",
    "# Reference\n",
    "\n",
    "The old (70%) code is left in the notebook for comparison. The new approach, as described above, achieves **100% extraction success** on the current dataset by respecting the quality of the input images and leveraging Tesseract's flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractname(img_path):\n",
    "    \n",
    "    # --- HELPER: TEXT CLEANER ---\n",
    "    def clean_text(raw_text):\n",
    "        if not raw_text: return \"\"\n",
    "        # Keep Arabic letters (0621-064A) and spaces\n",
    "        cleaned = re.sub(r'[^\\u0621-\\u064A\\s]', '', raw_text)\n",
    "        cleaned = cleaned.replace('\\n', ' ')\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        return cleaned\n",
    "\n",
    "    # --- LOAD IMAGE AS GRAYSCALE DIRECTLY ---\n",
    "    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_gray is None: \n",
    "        return \"\"\n",
    "\n",
    "    # Try multiple approaches and collect all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Preprocessing variants\n",
    "    preprocessed_images = {\n",
    "        'original': img_gray,\n",
    "        'padded': cv2.copyMakeBorder(img_gray, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255),\n",
    "    }\n",
    "    \n",
    "    # Add scaled version\n",
    "    h, w = img_gray.shape\n",
    "    scaled = cv2.resize(img_gray, (w*2, h*2), interpolation=cv2.INTER_CUBIC)\n",
    "    preprocessed_images['scaled_padded'] = cv2.copyMakeBorder(scaled, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255)\n",
    "    \n",
    "    # PSM modes to try\n",
    "    psm_modes = [6, 7, 3, 13]  # 13 = raw line\n",
    "    \n",
    "    for img_name, img in preprocessed_images.items():\n",
    "        for psm in psm_modes:\n",
    "            for oem in [3, 1]:  # Try both LSTM+Legacy and LSTM only\n",
    "                try:\n",
    "                    config = f\"--oem {oem} --psm {psm}\"\n",
    "                    text = pytesseract.image_to_string(img, lang='ara', config=config)\n",
    "                    cleaned = clean_text(text)\n",
    "                    \n",
    "                    if len(cleaned) > 2:\n",
    "                        all_results.append((cleaned, len(cleaned), img_name, psm, oem))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Return the longest valid result\n",
    "    if all_results:\n",
    "        all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return all_results[0][0]\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# folder_path = 'test_arabic_names_full'\n",
    "# data = []\n",
    "# if os.path.exists(folder_path):\n",
    "#     print(f\"Processing images in: {folder_path}...\\n\")\n",
    "    \n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         # Check if the file is an image\n",
    "#         if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "#             full_path = os.path.join(folder_path, filename)\n",
    "#             extracted_text = extractname(full_path)    \n",
    "#             clean_text_result = extracted_text.strip()\n",
    "#             data.append({'Filename': filename, 'Extracted Name': clean_text_result})\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # ===== SUCCESS RATE CALCULATION =====\n",
    "#     total_images = len(df)\n",
    "#     successful_extractions = len(df[df['Extracted Name'] != ''])\n",
    "#     failed_extractions = total_images - successful_extractions\n",
    "#     success_rate = (successful_extractions / total_images) * 100 if total_images > 0 else 0\n",
    "    \n",
    "#     print(f\"{'='*50}\")\n",
    "#     print(f\"OCR EXTRACTION RESULTS\")\n",
    "#     print(f\"{'='*50}\")\n",
    "#     print(f\"Total Images Processed: {total_images}\")\n",
    "#     print(f\"Successful Extractions: {successful_extractions}\")\n",
    "#     print(f\"Failed Extractions:     {failed_extractions}\")\n",
    "#     print(f\"Success Rate:           {success_rate:.2f}%\")\n",
    "#     print(f\"{'='*50}\\n\")\n",
    "    \n",
    "#     # Show failed images\n",
    "#     if failed_extractions > 0:\n",
    "#         failed_df = df[df['Extracted Name'] == '']\n",
    "#         print(\"Failed to extract text from:\")\n",
    "#         for idx, row in failed_df.iterrows():\n",
    "#             print(f\"  - {row['Filename']}\")\n",
    "#         print()\n",
    "    \n",
    "#     display(df.head(50))\n",
    "    \n",
    "# else:\n",
    "#     print(f\"the folder '{folder_path}' was not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117bf62",
   "metadata": {},
   "source": [
    "# **Main Pipeline** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40718274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# specific import for Jupyter Notebooks\n",
    "from IPython.display import display \n",
    "\n",
    "def main_pipeline():\n",
    "    base_dir = os.getcwd()\n",
    "    path_to_dataset = os.path.join(base_dir, 'Raw_IDs')\n",
    "    refrence_image_path = os.path.join(base_dir, 'Raw_IDs', 'ID14.jpg')\n",
    "    \n",
    "    # Ensure the classifier is trained\n",
    "    SVMclassifier = train_SVM_robust()\n",
    "    \n",
    "    data_for_excel = [] \n",
    "\n",
    "    # Safety check for directory\n",
    "    if not os.path.exists(path_to_dataset):\n",
    "        print(f\"Directory not found: {path_to_dataset}\")\n",
    "        return\n",
    "\n",
    "    for i in os.listdir(path_to_dataset):\n",
    "        if not i.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(path_to_dataset, i)\n",
    "        \n",
    "        # --- Processing ---\n",
    "        raw_img = cv2.imread(img_path)\n",
    "        aligned_img = align_images_sift(raw_img, refrence_image_path)\n",
    "        clean_img, is_impulsive = is_impulsive_noise(aligned_img)\n",
    "        clean_img, is_random = is_random_noise(clean_img)\n",
    "        name_img, digit_imgs, daf3_digits = extract_name_and_digits(clean_img)\n",
    "        \n",
    "        student_id = os.path.splitext(i)[0]\n",
    "        \n",
    "        # Save images\n",
    "        save_student_name(student_id, name_img)\n",
    "        save_split_digits(student_id, digit_imgs)\n",
    "        save_split_digits(f\"{student_id}_daf3\", daf3_digits, output_folder=\"extracted_daf3_digits\")\n",
    "        \n",
    "        # Predict Code\n",
    "        digit_preds = []\n",
    "        for digit_img in digit_imgs:\n",
    "            feat = extract_hog_features(digit_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            digit_preds.append(str(pred))\n",
    "        code_str = ''.join(digit_preds)\n",
    "        \n",
    "        # Predict Daf3\n",
    "        daf3_preds = []\n",
    "        for d_img in daf3_digits:\n",
    "            feat = extract_hog_features(d_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            daf3_preds.append(str(pred))\n",
    "        daf3_str = ''.join(daf3_preds)\n",
    "\n",
    "        # Extract Name\n",
    "        name_text = extractname(f'./extracted_names/{student_id}_name.jpg')\n",
    "        \n",
    "        # Add to list\n",
    "        data_for_excel.append({\n",
    "            \"Student ID\": student_id,\n",
    "            \"Name\": name_text,\n",
    "            \"Code\": code_str,\n",
    "            \"Daf3\": daf3_str,\n",
    "        })\n",
    "\n",
    "    # --- OUTPUT SECTION ---\n",
    "    if data_for_excel:\n",
    "        df = pd.DataFrame(data_for_excel)\n",
    "        df = df.sort_values(by=\"Student ID\", key=lambda x: x.str.extract(r'(\\d+)').iloc[:, 0].astype(int))\n",
    "\n",
    "        # 1. DISPLAY TABLE IN JUPYTER\n",
    "        print(\"Processing Complete. Results:\")\n",
    "        # This renders the DataFrame as a nice HTML table in the output cell\n",
    "        # We filter to show only Name, Code, and Daf3 as requested\n",
    "        display(df[['Student ID','Name', 'Code', 'Daf3']])\n",
    "        \n",
    "        # 2. SAVE TO EXCEL\n",
    "        output_file = \"Extracted_Results.xlsx\"\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "            \n",
    "            # Auto-resize columns\n",
    "            worksheet = writer.sheets['Sheet1']\n",
    "            for column in df:\n",
    "                column_length = max(df[column].astype(str).map(len).max(), len(column))\n",
    "                col_idx = df.columns.get_loc(column)\n",
    "                col_letter = chr(65 + col_idx)\n",
    "                worksheet.column_dimensions[col_letter].width = column_length + 2\n",
    "                \n",
    "        print(f\"Excel file saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No data processed.\")\n",
    "\n",
    "# Run it\n",
    "main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
