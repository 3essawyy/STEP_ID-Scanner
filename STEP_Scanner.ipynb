{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba08ac",
   "metadata": {},
   "source": [
    "## **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10cf11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pytesseract\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "from openpyxl.utils import get_column_letter  # Add this line\n",
    "from commonfunctions import *\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' # (Windows Example)\n",
    "\n",
    "\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab16d",
   "metadata": {},
   "source": [
    "## **Main Functions Overview**\n",
    "\n",
    "- **Image Alignment**\n",
    "  - Detects SIFT keypoints and descriptors in the input and reference images.\n",
    "  - Matches them using the ratio test.\n",
    "  - Uses RANSAC to estimate a homography.\n",
    "  - Applies the homography to warp the input image so it lines up with the reference.\n",
    "  - Returns the aligned image (or the original if not enough matches are found).\n",
    "\n",
    "- **Extract Details**\n",
    "  - Uses (x, y, w, h) coordinates to crop the aligned card into:\n",
    "    - The name region\n",
    "    - The code (ID) region\n",
    "  - Returns these sub-images for downstream OCR or digit processing.\n",
    "\n",
    "- **Save Student Name**\n",
    "  - Ensures the output folder exists.\n",
    "  - Writes the cropped name image to disk with a filename that includes the student ID.\n",
    "  - Creates a persistent record usable for manual review or OCR.\n",
    "\n",
    "- **Split and Save Digits**\n",
    "  - Converts the code region to grayscale and applies Otsu thresholding.\n",
    "  - Finds contours and filters out small noise.\n",
    "  - Selects the largest seven contours (by area) and sorts them left-to-right.\n",
    "  - Saves each detected digit crop into a per-student folder as individual image files.\n",
    "\n",
    "- **save_split_digits**\n",
    "  - Takes a list of digit images for a student.\n",
    "  - Ensures a folder exists for each student (named by their ID).\n",
    "  - Saves each digit image as `digit_0.jpg`, `digit_1.jpg`, ..., `digit_6.jpg` inside the student’s folder.\n",
    "  - Used for batch saving when all digit crops are already extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945bafe",
   "metadata": {},
   "source": [
    "## **Noise Detection and Treatment**\n",
    "- **Impulsive Noise (Median Filter)**\n",
    "- **Random Noise (Gaussian Filter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f54ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_random_noise(img, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Detects random noise and applies Non-Local Means (NLM) Denoising.\n",
    "    This is the best traditional filter for preserving textures and sharp edges.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Estimate noise level\n",
    "    stddev = np.std(img)\n",
    "    normalized_stddev = stddev / 255.0\n",
    "\n",
    "    if normalized_stddev < threshold:\n",
    "        return img, False\n",
    "\n",
    "    # cv2.fastNlMeansDenoising(src, h, templateWindowSize, searchWindowSize)\n",
    "    # h: Parameter deciding filter strength. Higher = more noise removal but less detail. \n",
    "    #    (10 is usually a good starting point for moderate noise)\n",
    "    # templateWindowSize: Size of the patches used to compute weights (typically 7).\n",
    "    # searchWindowSize: Size of the window to search for similar patches (typically 21).\n",
    "    \n",
    "    treated_img = cv2.fastNlMeansDenoising(\n",
    "        img, \n",
    "        None, \n",
    "        h=10, \n",
    "        templateWindowSize=7, \n",
    "        searchWindowSize=21\n",
    "    )\n",
    "    \n",
    "    return treated_img, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53737cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_impulsive_noise(img, threshold=0.1, black_range=(0, 9), white_range=(246, 255)):\n",
    "    \"\"\"\n",
    "    Detects impulsive noise and applies a Decision-Based Median Filter.\n",
    "    Only noisy pixels are modified; clean pixels remain sharp and unblurred.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    total_pixels = img.size\n",
    "    # Create masks for pixels that fall into the 'salt' or 'pepper' ranges\n",
    "    is_pepper = (img >= black_range[0]) & (img <= black_range[1])\n",
    "    is_salt = (img >= white_range[0]) & (img <= white_range[1])\n",
    "    \n",
    "    # Combined noise mask\n",
    "    noise_mask = is_pepper | is_salt\n",
    "    num_noise_pixels = np.sum(noise_mask)\n",
    "    prop = num_noise_pixels / total_pixels\n",
    "\n",
    "    if prop < threshold:\n",
    "        return img, False \n",
    "\n",
    "    # Determine kernel size based on noise severity\n",
    "    k = int(3 + prop * 10)\n",
    "    if k % 2 == 0: k += 1\n",
    "    k = min(max(k, 3), 9)\n",
    "\n",
    "    # 1. Apply a standard median blur to a temporary image\n",
    "    median_filtered = cv2.medianBlur(img, k)\n",
    "\n",
    "    # 2. DECISION STEP: Create output as a copy of the original\n",
    "    # This ensures we don't blur the whole image.\n",
    "    treated_img = img.copy()\n",
    "\n",
    "    # 3. Only replace pixels where the noise_mask is True\n",
    "    treated_img[noise_mask] = median_filtered[noise_mask]\n",
    "\n",
    "    return treated_img, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba8fd2",
   "metadata": {},
   "source": [
    "Contrast enhancment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0082918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def enhance_contrast_clahe(img, clip_limit=2.0, tile_size=(8, 8)):\n",
    "    \"\"\"\n",
    "    Applies CLAHE for high-quality contrast enhancement.\n",
    "    Returns the enhanced image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        # Convert to LAB to enhance only the Luminance (L) channel\n",
    "        # This prevents color distortion.\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Create CLAHE object\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "        cl = clahe.apply(l)\n",
    "        \n",
    "        # Merge back and convert to BGR\n",
    "        enhanced_lab = cv2.merge((cl, a, b))\n",
    "        return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "    else:\n",
    "        # Grayscale implementation\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "        return clahe.apply(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf03c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images_sift(img_to_align, reference_path):\n",
    "    img1 = img_to_align\n",
    "    img2 = cv2.imread(reference_path)      # Train Image (The perfect template)\n",
    "    \n",
    "    # --- FIX: Check if img1 is already grayscale ---\n",
    "    if len(img1.shape) == 3:\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray1 = img1 # Already grayscale\n",
    "\n",
    "    # Ref image is loaded from disk, usually BGR, but good to check\n",
    "    if len(img2.shape) == 3:\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray2 = img2\n",
    "\n",
    "    sift = cv2.SIFT_create() \n",
    "    \n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        h, w = img2.shape[:2]\n",
    "        # Warp the original img1 (whether gray or color)\n",
    "        aligned_img = cv2.warpPerspective(img1, M, (w, h))\n",
    "\n",
    "        return aligned_img\n",
    "    \n",
    "    else:\n",
    "        print(f\"Not enough matches found: {len(good_matches)}/10\")\n",
    "        return img1\n",
    "    \n",
    "\n",
    "\n",
    "def extract_details(aligned_image):\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    \n",
    "    name_contour = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_contour = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    \n",
    "    return name_contour, code_contour\n",
    "\n",
    "\n",
    "def save_student_name(student_id, name_img, output_folder=\"extracted_names\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    # Construct filename: extracted_names/ID1_name.jpg\n",
    "    filename = f\"{output_folder}/{student_id}_name.jpg\"\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(filename, name_img)\n",
    "    \n",
    "\n",
    "def split_and_save_digits(student_id, code_roi, output_folder=\"extracted_digits\"):\n",
    "    save_path = f\"{output_folder}/ID{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    gray = cv2.cvtColor(code_roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # A. Collect all valid candidates\n",
    "    candidates = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        # Filter tiny noise\n",
    "        if h > 15 and w > 5:\n",
    "            candidates.append((x, y, w, h, area))\n",
    "            \n",
    "    # B. CRITICAL: Select exactly the top 7 by AREA (Size)\n",
    "    # This removes small specks or the colon \":\" if it was caught\n",
    "    candidates = sorted(candidates, key=lambda c: c[4], reverse=True) # Sort largest first\n",
    "    final_digits = candidates[:7] # Take top 7\n",
    "    \n",
    "    # C. Sort the final 7 by X-COORDINATE (Left -> Right)\n",
    "    # This puts them back in the correct reading order (1, 2, 3...)\n",
    "    final_digits = sorted(final_digits, key=lambda c: c[0])\n",
    "        \n",
    "    # D. Save\n",
    "    for index, (x, y, w, h, area) in enumerate(final_digits):\n",
    "        digit_img = code_roi[y:y+h, x:x+w]\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)\n",
    "\n",
    "      \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_name_and_digits(aligned_image):\n",
    "    \"\"\"\n",
    "    Input: An aligned ID card image.\n",
    "    Output: \n",
    "      - name_roi: The image of the extracted name.\n",
    "      - code_digits: A list of images for the code digits.\n",
    "      - daf3_digits: A list of images for the daf3 digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Hardcoded Coordinates ---\n",
    "    # x, y, w, h\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    daf3_coords = (350, 500, 620, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    dx, dy, dw, dh = daf3_coords\n",
    "    \n",
    "    # Extract ROIs\n",
    "    name_img = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_roi = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    daf3_img = aligned_image[dy:dy+dh, dx:dx+dw]\n",
    "    \n",
    "    # --- Helper Function to Process Any ROI ---\n",
    "    def process_roi_digits(roi_img, digit_limit):\n",
    "        \"\"\"\n",
    "        Applies grayscale, thresholding, contour detection, \n",
    "        splitting of merged digits, and sorting.\n",
    "        \"\"\"\n",
    "        #gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if len(roi_img.shape) == 3:\n",
    "            gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = roi_img\n",
    "        # --- FIX ENDS HERE --\n",
    "        \n",
    "        # Binary Inverse + Otsu\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        candidates = []\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            area = w * h\n",
    "            \n",
    "            # Filter tiny noise \n",
    "            if h > 15 and w > 5:\n",
    "                \n",
    "                # --- CHECK FOR MERGED DIGITS ---\n",
    "                # If width > 0.8 * height, likely two digits stuck together\n",
    "                if w > 0.8 * h: \n",
    "                    half_w = w // 2\n",
    "                    # Digit 1 (Left half)\n",
    "                    candidates.append((x, y, half_w, h, half_w * h))\n",
    "                    # Digit 2 (Right half)\n",
    "                    candidates.append((x + half_w, y, half_w, h, half_w * h))\n",
    "                else:\n",
    "                    # Normal single digit\n",
    "                    candidates.append((x, y, w, h, area))\n",
    "        \n",
    "        # 1. Sort by Area Descending (Keep only the largest objects to remove noise)\n",
    "        candidates = sorted(candidates, key=lambda c: c[4], reverse=True)[:digit_limit]\n",
    "        \n",
    "        # 2. Sort by X-coordinate Ascending (Order them Left -> Right)\n",
    "        final_candidates = sorted(candidates, key=lambda c: c[0])\n",
    "        \n",
    "        # Crop the actual images\n",
    "        cropped_digits = []\n",
    "        for (x, y, w, h, area) in final_candidates:\n",
    "            digit_crop = roi_img[y:y+h, x:x+w]\n",
    "            cropped_digits.append(digit_crop)\n",
    "            \n",
    "        return cropped_digits\n",
    "\n",
    "    # --- 2. Process Regions ---\n",
    "    \n",
    "    # Detect Code (Limit 7 digits)\n",
    "    code_digits = process_roi_digits(code_roi, digit_limit=7)\n",
    "    \n",
    "    # Detect Daf3 (Limit 14 digits)\n",
    "    daf3_digits = process_roi_digits(daf3_img, digit_limit=14)\n",
    "\n",
    "    return name_img, code_digits, daf3_digits\n",
    "\n",
    "def save_split_digits(student_id, digit_imgs, output_folder=\"extracted_digits\"):\n",
    "    \"\"\"\n",
    "    Saves a list of digit images for a student in the same way as split_and_save_digits.\n",
    "    Each digit is saved as digit_0.jpg, digit_1.jpg, ..., digit_6.jpg in a folder per student.\n",
    "    \"\"\"\n",
    "    save_path = f\"{output_folder}/{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for index, digit_img in enumerate(digit_imgs):\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f17014",
   "metadata": {},
   "source": [
    "## **SVM English Number Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff7086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_dataset = r\"train_digits\"  # Training set\n",
    "\n",
    "def train_SVM_robust():\n",
    "    # 1. Map your specific filename prefixes to actual digits\n",
    "    label_map = {\n",
    "        'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', \n",
    "        'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '9'\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    img_filenames = os.listdir(path_to_train_dataset)\n",
    "    print(f\"Loading {len(img_filenames)} training images...\")\n",
    "\n",
    "    for fn in img_filenames:\n",
    "        if not fn.lower().endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "\n",
    "        # Get the first letter (a, b, c...)\n",
    "        prefix = fn[0].lower()\n",
    "        if prefix in label_map:\n",
    "            labels.append(label_map[prefix])\n",
    "            \n",
    "            path = os.path.join(path_to_train_dataset, fn)\n",
    "            img = cv2.imread(path)\n",
    "            \n",
    "            # Extract HOG features (ensure preprocessing matches)\n",
    "            features.append(extract_hog_features(img))\n",
    "    \n",
    "    # 2. Create a Pipeline: Scale Features -> Train SVM\n",
    "    # Scaling is CRITICAL for HOG-based SVMs\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', LinearSVC(random_state=42, max_iter=5000, dual=False))\n",
    "    ])\n",
    "    \n",
    "    # 3. Train/Test Split for internal validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(f\"Training Complete. Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    # Ensure grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # APPLY IDENTICAL PREPROCESSING TO TRAINING AND TEST DATA\n",
    "    # This turns both sets into \"binary masks\" to ignore lighting/shadows\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32)) # target_img_size\n",
    "    \n",
    "    win_size = (32, 32)\n",
    "    cell_size = (8, 8)  # Slightly larger cells help ignore \"noise/shadows\"\n",
    "    block_size = (16, 16)\n",
    "    block_stride = (8, 8)\n",
    "    nbins = 9\n",
    "    \n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fbfe0",
   "metadata": {},
   "source": [
    "# **Tesseract Arabic OCR**\n",
    "\n",
    "## **Current Situation**\n",
    "\n",
    "The project uses Tesseract OCR to extract Arabic names from scanned images. Initially, the extraction pipeline achieved only a **70% success rate**. This meant that about 30% of the images failed to yield any valid Arabic text, even though the images were visually clear and contained readable names.\n",
    "\n",
    "## **Why Was the Success Rate Only 70%?**\n",
    "\n",
    "- **Overprocessing:** The original code applied several preprocessing steps (scaling, thresholding, blurring, etc.) before running OCR. While these steps can help with noisy or low-contrast images, they often **destroy clean, high-contrast text**—especially for Arabic, where fine details matter.\n",
    "- **Order of Operations:** The pipeline tried processed versions first, so if the original image was already optimal, it was never used for OCR.\n",
    "- **PSM/OEM Settings:** The code tried a limited set of Tesseract Page Segmentation Modes (PSM) and OCR Engine Modes (OEM), which may not have been optimal for all images.\n",
    "- **Text Cleaning:** The cleaning function was aggressive, but if Tesseract output was empty or too short, the result was discarded.\n",
    "\n",
    "## **What Was Changed to Achieve 100% Success**\n",
    "\n",
    "1. **Prioritize the Original Image:**  \n",
    "   The new code always tries the original, unprocessed grayscale image first, with several PSM settings. This ensures that clean images are not degraded by unnecessary processing.\n",
    "\n",
    "2. **Expanded Preprocessing (But Only If Needed):**  \n",
    "   Only if the original image fails, the code tries padded and scaled versions, but never applies destructive thresholding or blurring unless absolutely necessary.\n",
    "\n",
    "3. **Multiple PSM and OEM Combinations:**  \n",
    "   For each image variant, the code tries several PSM (6, 7, 3, 13) and both OEM (3, 1) settings, maximizing the chance that Tesseract will interpret the layout correctly.\n",
    "\n",
    "4. **Result Selection:**  \n",
    "   All non-empty results are collected, and the **longest valid extraction** is chosen, which is usually the correct full name.\n",
    "\n",
    "5. **Diagnostics:**  \n",
    "   Additional debug and diagnostic code was used to confirm that the original image, with minimal processing, consistently yields the best results for this dataset.\n",
    "\n",
    "# Reference\n",
    "\n",
    "The old (70%) code is left in the notebook for comparison. The new approach, as described above, achieves **100% extraction success** on the current dataset by respecting the quality of the input images and leveraging Tesseract's flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e532a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "# def extractname(img_path):\n",
    "    \n",
    "#     # --- HELPER: TEXT CLEANER ---\n",
    "#     def clean_text(raw_text):\n",
    "#         if not raw_text: return \"\"\n",
    "#         # Keep Arabic letters (0621-064A) and spaces\n",
    "#         cleaned = re.sub(r'[^\\u0621-\\u064A\\s]', '', raw_text)\n",
    "#         cleaned = cleaned.replace('\\n', ' ')\n",
    "#         cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "#         return cleaned\n",
    "\n",
    "#     # --- LOAD IMAGE AS GRAYSCALE DIRECTLY ---\n",
    "#     img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     if img_gray is None: \n",
    "#         return \"\"\n",
    "\n",
    "#     # Try multiple approaches and collect all results\n",
    "#     all_results = []\n",
    "    \n",
    "#     # Preprocessing variants\n",
    "#     preprocessed_images = {\n",
    "#         'original': img_gray,\n",
    "#         'padded': cv2.copyMakeBorder(img_gray, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255),\n",
    "#     }\n",
    "    \n",
    "#     # Add scaled version\n",
    "#     h, w = img_gray.shape\n",
    "#     scaled = cv2.resize(img_gray, (w*2, h*2), interpolation=cv2.INTER_CUBIC)\n",
    "#     preprocessed_images['scaled_padded'] = cv2.copyMakeBorder(scaled, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255)\n",
    "    \n",
    "#     # PSM modes to try\n",
    "#     psm_modes = [6, 7, 3, 13]  # 13 = raw line\n",
    "    \n",
    "#     for img_name, img in preprocessed_images.items():\n",
    "#         for psm in psm_modes:\n",
    "#             for oem in [3, 1]:  # Try both LSTM+Legacy and LSTM only\n",
    "#                 try:\n",
    "#                     config = f\"--oem {oem} --psm {psm}\"\n",
    "#                     text = pytesseract.image_to_string(img, lang='ara', config=config)\n",
    "#                     cleaned = clean_text(text)\n",
    "                    \n",
    "#                     if len(cleaned) > 2:\n",
    "#                         all_results.append((cleaned, len(cleaned), img_name, psm, oem))\n",
    "#                 except:\n",
    "#                     continue\n",
    "    \n",
    "#     # Return the longest valid result\n",
    "#     if all_results:\n",
    "#         all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "#         return all_results[0][0]\n",
    "    \n",
    "#     return \"\"\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['ar', 'en'], gpu=True)\n",
    "\n",
    "def extractname(image_path):\n",
    "    \"\"\"\n",
    "    Reads text from the image path using EasyOCR.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # detail=0 returns a list of strings directly\n",
    "        # paragraph=True helps combine text lines if the name is split\n",
    "        results = reader.readtext(image_path, detail=0, paragraph=True)\n",
    "        \n",
    "        # Join all detected text parts into one string\n",
    "        full_name = \" \".join(results)\n",
    "        return full_name.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR Error on {image_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1acdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_pipeline_accuracy(true_file_path, extracted_file_path):\n",
    "    # 1. Load the Excel files\n",
    "    try:\n",
    "        df_true = pd.read_excel(true_file_path)\n",
    "        df_extracted = pd.read_excel(extracted_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Align the data lengths (process only up to the shortest file)\n",
    "    min_len = min(len(df_true), len(df_extracted))\n",
    "    df_true = df_true.iloc[:min_len].reset_index(drop=True)\n",
    "    df_extracted = df_extracted.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "    # Define the columns to check\n",
    "    columns_to_check = ['Code', 'Daf3', 'Name']\n",
    "    scores = {}\n",
    "\n",
    "    print(f\"--- Accuracy Report (Checking {min_len} rows) ---\\n\")\n",
    "\n",
    "    for col in columns_to_check:\n",
    "        # Check if column exists in both sheets\n",
    "        if col not in df_true.columns or col not in df_extracted.columns:\n",
    "            print(f\"Error: Column '{col}' missing from one of the files.\")\n",
    "            scores[col] = 0.0\n",
    "            continue\n",
    "\n",
    "        # --- Data Normalization ---\n",
    "        true_series = df_true[col].astype(str).fillna('')\n",
    "        extracted_series = df_extracted[col].astype(str).fillna('')\n",
    "\n",
    "        # Basic Cleanup\n",
    "        true_clean = true_series.str.strip().str.replace(r'\\.0$', '', regex=True)\n",
    "        extracted_clean = extracted_series.str.strip().str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "        # --- COMPARISON LOGIC ---\n",
    "        \n",
    "        if col == 'Name':\n",
    "            # === NEW: WORD-BY-WORD COMPARISON FOR NAMES ===\n",
    "            # We calculate a score (0.0 to 1.0) for each row\n",
    "            row_scores = []\n",
    "            \n",
    "            for t_val, e_val in zip(true_clean, extracted_clean):\n",
    "                # 1. Split names into sets of words\n",
    "                t_words = set(t_val.split())\n",
    "                e_words = set(e_val.split())\n",
    "                \n",
    "                # 2. Calculate intersection (common words)\n",
    "                if len(t_words) == 0:\n",
    "                    # If ground truth is empty, score is 1 if extracted is also empty\n",
    "                    row_scores.append(1.0 if len(e_words) == 0 else 0.0)\n",
    "                else:\n",
    "                    common = t_words.intersection(e_words)\n",
    "                    # Score = correct_words_found / total_actual_words\n",
    "                    score = len(common) / len(t_words)\n",
    "                    row_scores.append(score)\n",
    "            \n",
    "            # Average the row scores\n",
    "            accuracy = np.mean(row_scores) * 100\n",
    "            \n",
    "        else:\n",
    "            # === STRICT COMPARISON FOR CODES/NUMBERS ===\n",
    "            matches = (true_clean == extracted_clean)\n",
    "            accuracy = (matches.sum() / len(matches)) * 100\n",
    "\n",
    "        scores[col] = accuracy\n",
    "        print(f\"{col} Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # 3. Calculate Average\n",
    "    if scores:\n",
    "        average_accuracy = sum(scores.values()) / len(scores)\n",
    "    else:\n",
    "        average_accuracy = 0.0\n",
    "\n",
    "    print(f\"\\n--------------------------------\")\n",
    "    print(f\"AVERAGE ACCURACY: {average_accuracy:.2f}%\")\n",
    "    print(f\"--------------------------------\")\n",
    "\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbbc85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import easyocr\n",
    "\n",
    "# Initialize the reader for Arabic and English\n",
    "# The model weights will download automatically on the first run\n",
    "#reader = easyocr.Reader(['ar', 'en'], gpu=True) # Set gpu=False if you don't have one\n",
    "\n",
    "# Perform OCR on an image\n",
    "#results = reader.readtext('path_to_your_image.jpg')\n",
    "\n",
    "# Print the recognized text\n",
    "#for (bbox, text, prob) in results:\n",
    "    #print(f\"Text: {text} (Confidence: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117bf62",
   "metadata": {},
   "source": [
    "# **Main Pipeline** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40718274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 149 training images...\n",
      "Training Complete. Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Complete. Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Code</th>\n",
       "      <th>Daf3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1</td>\n",
       "      <td>مصطفى محمود احمد عويس</td>\n",
       "      <td>1200277</td>\n",
       "      <td>14712020100041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ID2</td>\n",
       "      <td>شاميه علاء محمد سيد احمد</td>\n",
       "      <td>1220175</td>\n",
       "      <td>14712022101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ID3</td>\n",
       "      <td>اسمهان ابراهيم يوسف البيطار</td>\n",
       "      <td>1220165</td>\n",
       "      <td>14712022101214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ID4</td>\n",
       "      <td>حلمي علي ريان</td>\n",
       "      <td>1220145</td>\n",
       "      <td>14712022101495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ID5</td>\n",
       "      <td>حسين محمد ماهر بهاءالدين</td>\n",
       "      <td>1220237</td>\n",
       "      <td>14712022101358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ID6</td>\n",
       "      <td>متحمد منير عبدالحميد كمال</td>\n",
       "      <td>4230174</td>\n",
       "      <td>14712022101524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ID7</td>\n",
       "      <td>يوسف محمد حمدى محمل عثمان</td>\n",
       "      <td>1220301</td>\n",
       "      <td>14712522101392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ID8</td>\n",
       "      <td>معاذ امام احمد امام احمد</td>\n",
       "      <td>1220205</td>\n",
       "      <td>14712022101329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ID9</td>\n",
       "      <td>نغم طارق محمد احمد قاسم</td>\n",
       "      <td>1220149</td>\n",
       "      <td>14712022101620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10</td>\n",
       "      <td>يوسف عمرو سيد كمال محمود محمد</td>\n",
       "      <td>1220319</td>\n",
       "      <td>14712022101322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID11</td>\n",
       "      <td>يوسف محمد حمزه محمد عبدالتواب</td>\n",
       "      <td>1220092</td>\n",
       "      <td>14712022100089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID12</td>\n",
       "      <td>عمر محمد محمود صفوت احمد سعد الدين</td>\n",
       "      <td>1220188</td>\n",
       "      <td>14712022100080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID13</td>\n",
       "      <td>عبد الرحمن عل السيد محمد السيد الحديدى</td>\n",
       "      <td>1220062</td>\n",
       "      <td>14712022100036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID14</td>\n",
       "      <td>محمد تامر عبدالعزيز سالم</td>\n",
       "      <td>4230189</td>\n",
       "      <td>14712022101553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID15</td>\n",
       "      <td>فادى سامى نبيل داود</td>\n",
       "      <td>4230160</td>\n",
       "      <td>1471202210078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID16</td>\n",
       "      <td>احمد مدحت عبدالعزيز على عوض</td>\n",
       "      <td>4230142</td>\n",
       "      <td>14712022101079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID17</td>\n",
       "      <td>مروان عمرو عبدالمجبد فواد احمد تكرى</td>\n",
       "      <td>1220279</td>\n",
       "      <td>14712022101373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID18</td>\n",
       "      <td>عبدالله محمد جمال الدين احمد مصطفى</td>\n",
       "      <td>1220256</td>\n",
       "      <td>14712022101354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ID19</td>\n",
       "      <td>رغد وليد محمد نبيه فرج حسن</td>\n",
       "      <td>1220123</td>\n",
       "      <td>14712022101537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ID20</td>\n",
       "      <td>كريم تحمد نجم الدين حمدي عفيفى</td>\n",
       "      <td>1220137</td>\n",
       "      <td>14712022101624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID21</td>\n",
       "      <td>عمرو ايهاب مختار فرحات</td>\n",
       "      <td>4230159</td>\n",
       "      <td>14712022101610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ID22</td>\n",
       "      <td>سارة محمد مصطفى جوده زهير</td>\n",
       "      <td>1220125</td>\n",
       "      <td>14712022101262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ID23</td>\n",
       "      <td>يوسف جلال محمد نور الدين جلال</td>\n",
       "      <td>1200309</td>\n",
       "      <td>14712020100046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ID24</td>\n",
       "      <td>مجمد تامر عبدالعزيز سالم</td>\n",
       "      <td>4230189</td>\n",
       "      <td>14712022101553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ID25</td>\n",
       "      <td>يوسف احمد عفيفى منصور</td>\n",
       "      <td>1200883</td>\n",
       "      <td>15057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ID26</td>\n",
       "      <td>عمر ابراهيم احمد ابراهيم السردى</td>\n",
       "      <td>1190533</td>\n",
       "      <td>17120191017740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ID27</td>\n",
       "      <td>اية  احمد محمد خشبه</td>\n",
       "      <td>1230165</td>\n",
       "      <td>14712023101223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ID28</td>\n",
       "      <td>احمد هشام سيل سالم</td>\n",
       "      <td>1220003</td>\n",
       "      <td>14712022100623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ID29</td>\n",
       "      <td>حنين محمد اسماعيل محمد حماده</td>\n",
       "      <td>1220121</td>\n",
       "      <td>14712022101552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ID30</td>\n",
       "      <td>امير سليم سليمان سىليمان موسى</td>\n",
       "      <td>1210096</td>\n",
       "      <td>14712021100205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ID31</td>\n",
       "      <td>دمد عبدالر حيم سالم عبدالهادي</td>\n",
       "      <td>1230315</td>\n",
       "      <td>14712023101217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ID32</td>\n",
       "      <td>حسن حسام حسن حامد عد الفتاح</td>\n",
       "      <td>1210216</td>\n",
       "      <td>14712021101550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ID33</td>\n",
       "      <td>زياد ايهاب محمد ممدوح نافع</td>\n",
       "      <td>1210227</td>\n",
       "      <td>14712021101455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ID34</td>\n",
       "      <td>ابة احمد محمد خشبه</td>\n",
       "      <td>1230165</td>\n",
       "      <td>14712023101223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ID35</td>\n",
       "      <td>سلمى اجمد عبد الرحيم متحمل</td>\n",
       "      <td>1210235</td>\n",
       "      <td>14712021101553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ID36</td>\n",
       "      <td>ابراهيم محمود ابراهيم عبد المعطى</td>\n",
       "      <td>1180568</td>\n",
       "      <td>14712018102167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ID37</td>\n",
       "      <td>يحي محمود نصر محمود</td>\n",
       "      <td>100</td>\n",
       "      <td>14071020101893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ID38</td>\n",
       "      <td>سلمى محمد ابرا هيم فتحى ابوريده</td>\n",
       "      <td>1230196</td>\n",
       "      <td>14712023101286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ID39</td>\n",
       "      <td>مروان عمرو عبدالمجيد فؤاد احمد شكرى</td>\n",
       "      <td>1220279</td>\n",
       "      <td>14712022101373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ID40</td>\n",
       "      <td>حسن عمرو مصطفي محمد يوسف</td>\n",
       "      <td>1230030</td>\n",
       "      <td>14712023101060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ID41</td>\n",
       "      <td>نلد سامح صبحي حسن صبح</td>\n",
       "      <td>127460</td>\n",
       "      <td>14712020102030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ID42</td>\n",
       "      <td>محمد احمد فهيم عبد الفتاح</td>\n",
       "      <td>1220273</td>\n",
       "      <td>14712022101338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ID43</td>\n",
       "      <td>أحمد محمد سيد صابر محمد</td>\n",
       "      <td>1230144</td>\n",
       "      <td>14712023100148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ID44</td>\n",
       "      <td>احمد محمد حسين الكومى</td>\n",
       "      <td>1220221</td>\n",
       "      <td>14712022101628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ID45</td>\n",
       "      <td>لينه بهيج محروس بهيج</td>\n",
       "      <td>1210284</td>\n",
       "      <td>14712021101365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ID46</td>\n",
       "      <td>جمانة عمرو مصطفى عبلا الصالح عرابي</td>\n",
       "      <td>1220229</td>\n",
       "      <td>1471202210174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ID47</td>\n",
       "      <td>احمد سامح صلاح مصيلحى</td>\n",
       "      <td>4230138</td>\n",
       "      <td>14712022101505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ID48</td>\n",
       "      <td>ادهم احمد محمود ممدو ح شبانة</td>\n",
       "      <td>1230157</td>\n",
       "      <td>14712023101281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ID49</td>\n",
       "      <td>جنى ايمن وفائى مجمد عيتت</td>\n",
       "      <td>1210349</td>\n",
       "      <td>14712021101568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student ID                                    Name     Code            Daf3\n",
       "0         ID1                   مصطفى محمود احمد عويس  1200277  14712020100041\n",
       "11        ID2                شاميه علاء محمد سيد احمد  1220175     14712022101\n",
       "22        ID3             اسمهان ابراهيم يوسف البيطار  1220165  14712022101214\n",
       "33        ID4                           حلمي علي ريان  1220145  14712022101495\n",
       "44        ID5                حسين محمد ماهر بهاءالدين  1220237  14712022101358\n",
       "45        ID6               متحمد منير عبدالحميد كمال  4230174  14712022101524\n",
       "46        ID7               يوسف محمد حمدى محمل عثمان  1220301  14712522101392\n",
       "47        ID8                معاذ امام احمد امام احمد  1220205  14712022101329\n",
       "48        ID9                 نغم طارق محمد احمد قاسم  1220149  14712022101620\n",
       "1        ID10           يوسف عمرو سيد كمال محمود محمد  1220319  14712022101322\n",
       "2        ID11           يوسف محمد حمزه محمد عبدالتواب  1220092  14712022100089\n",
       "3        ID12      عمر محمد محمود صفوت احمد سعد الدين  1220188  14712022100080\n",
       "4        ID13  عبد الرحمن عل السيد محمد السيد الحديدى  1220062  14712022100036\n",
       "5        ID14                محمد تامر عبدالعزيز سالم  4230189  14712022101553\n",
       "6        ID15                     فادى سامى نبيل داود  4230160   1471202210078\n",
       "7        ID16             احمد مدحت عبدالعزيز على عوض  4230142  14712022101079\n",
       "8        ID17     مروان عمرو عبدالمجبد فواد احمد تكرى  1220279  14712022101373\n",
       "9        ID18      عبدالله محمد جمال الدين احمد مصطفى  1220256  14712022101354\n",
       "10       ID19              رغد وليد محمد نبيه فرج حسن  1220123  14712022101537\n",
       "12       ID20          كريم تحمد نجم الدين حمدي عفيفى  1220137  14712022101624\n",
       "13       ID21                  عمرو ايهاب مختار فرحات  4230159  14712022101610\n",
       "14       ID22               سارة محمد مصطفى جوده زهير  1220125  14712022101262\n",
       "15       ID23           يوسف جلال محمد نور الدين جلال  1200309  14712020100046\n",
       "16       ID24                مجمد تامر عبدالعزيز سالم  4230189  14712022101553\n",
       "17       ID25                   يوسف احمد عفيفى منصور  1200883           15057\n",
       "18       ID26         عمر ابراهيم احمد ابراهيم السردى  1190533  17120191017740\n",
       "19       ID27                     اية  احمد محمد خشبه  1230165  14712023101223\n",
       "20       ID28                      احمد هشام سيل سالم  1220003  14712022100623\n",
       "21       ID29            حنين محمد اسماعيل محمد حماده  1220121  14712022101552\n",
       "23       ID30           امير سليم سليمان سىليمان موسى  1210096  14712021100205\n",
       "24       ID31           دمد عبدالر حيم سالم عبدالهادي  1230315  14712023101217\n",
       "25       ID32             حسن حسام حسن حامد عد الفتاح  1210216  14712021101550\n",
       "26       ID33              زياد ايهاب محمد ممدوح نافع  1210227  14712021101455\n",
       "27       ID34                      ابة احمد محمد خشبه  1230165  14712023101223\n",
       "28       ID35              سلمى اجمد عبد الرحيم متحمل  1210235  14712021101553\n",
       "29       ID36        ابراهيم محمود ابراهيم عبد المعطى  1180568  14712018102167\n",
       "30       ID37                     يحي محمود نصر محمود      100  14071020101893\n",
       "31       ID38         سلمى محمد ابرا هيم فتحى ابوريده  1230196  14712023101286\n",
       "32       ID39     مروان عمرو عبدالمجيد فؤاد احمد شكرى  1220279  14712022101373\n",
       "34       ID40                حسن عمرو مصطفي محمد يوسف  1230030  14712023101060\n",
       "35       ID41                   نلد سامح صبحي حسن صبح   127460  14712020102030\n",
       "36       ID42               محمد احمد فهيم عبد الفتاح  1220273  14712022101338\n",
       "37       ID43                 أحمد محمد سيد صابر محمد  1230144  14712023100148\n",
       "38       ID44                   احمد محمد حسين الكومى  1220221  14712022101628\n",
       "39       ID45                    لينه بهيج محروس بهيج  1210284  14712021101365\n",
       "40       ID46      جمانة عمرو مصطفى عبلا الصالح عرابي  1220229   1471202210174\n",
       "41       ID47                   احمد سامح صلاح مصيلحى  4230138  14712022101505\n",
       "42       ID48            ادهم احمد محمود ممدو ح شبانة  1230157  14712023101281\n",
       "43       ID49                جنى ايمن وفائى مجمد عيتت  1210349  14712021101568"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved to: Extracted_Results.xlsx\n",
      "--- Accuracy Report (Checking 49 rows) ---\n",
      "\n",
      "Code Accuracy: 93.88%\n",
      "Daf3 Accuracy: 83.67%\n",
      "Name Accuracy: 85.85%\n",
      "\n",
      "--------------------------------\n",
      "AVERAGE ACCURACY: 87.80%\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython.display import display, Image as IPyImage, HTML\n",
    "\n",
    "def show_image_cv(img, title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    if len(img.shape) == 2:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main_pipeline():\n",
    "    base_dir = os.getcwd()\n",
    "    path_to_dataset = os.path.join(base_dir, 'Raw_IDs')\n",
    "    refrence_image_path = os.path.join(base_dir, 'Raw_IDs', 'ID14.jpg')\n",
    "\n",
    "    # Ensure the classifier is trained\n",
    "    SVMclassifier = train_SVM_robust()\n",
    "\n",
    "    data_for_excel = []\n",
    "\n",
    "    # Safety check for directory\n",
    "    if not os.path.exists(path_to_dataset):\n",
    "        print(f\"Directory not found: {path_to_dataset}\")\n",
    "        return\n",
    "\n",
    "    for i in os.listdir(path_to_dataset):\n",
    "        if not i.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(path_to_dataset, i)\n",
    "\n",
    "        # --- Processing ---\n",
    "        raw_img = cv2.imread(img_path)\n",
    "        clean_img, is_impulsive = is_impulsive_noise(raw_img)\n",
    "        aligned_img = align_images_sift(clean_img, refrence_image_path)\n",
    "        # clean_img2, is_random = is_random_noise(aligned_img)\n",
    "        enhanced_img = enhance_contrast_clahe(aligned_img)\n",
    "        name_img, digit_imgs, daf3_digits = extract_name_and_digits(enhanced_img)\n",
    "        student_id = os.path.splitext(i)[0]\n",
    "\n",
    "        # Save images\n",
    "        save_student_name(student_id, name_img)\n",
    "        save_split_digits(student_id, digit_imgs)\n",
    "        save_split_digits(f\"{student_id}_daf3\", daf3_digits, output_folder=\"extracted_daf3_digits\")\n",
    "\n",
    "        # Predict Code\n",
    "        digit_preds = []\n",
    "        for digit_img in digit_imgs:\n",
    "            feat = extract_hog_features(digit_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            digit_preds.append(str(pred))\n",
    "        code_str = ''.join(digit_preds)\n",
    "\n",
    "        # Predict Daf3\n",
    "        daf3_preds = []\n",
    "        for d_img in daf3_digits:\n",
    "            feat = extract_hog_features(d_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            daf3_preds.append(str(pred))\n",
    "        daf3_str = ''.join(daf3_preds)\n",
    "\n",
    "        # Extract Name\n",
    "        name_text = extractname(f'./extracted_names/{student_id}_name.jpg')\n",
    "\n",
    "        # Add to list\n",
    "        data_for_excel.append({\n",
    "            \"Student ID\": student_id,\n",
    "            \"Name\": name_text,\n",
    "            \"Code\": code_str,\n",
    "            \"Daf3\": daf3_str,\n",
    "        })\n",
    "\n",
    "    # --- OUTPUT SECTION ---\n",
    "    if data_for_excel:\n",
    "        df = pd.DataFrame(data_for_excel)\n",
    "        # Sort by Student ID number if needed\n",
    "        df = df.sort_values(by=\"Student ID\", key=lambda x: x.str.extract(r'(\\d+)').iloc[:, 0].astype(int))\n",
    "\n",
    "        # 1. DISPLAY TABLE IN JUPYTER (with Student ID)\n",
    "        print(\"Processing Complete. Results:\")\n",
    "        display(df[['Student ID', 'Name', 'Code', 'Daf3']])\n",
    "\n",
    "        # 2. SAVE TO EXCEL\n",
    "        output_file = \"Extracted_Results.xlsx\"\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "\n",
    "            # Auto-resize columns\n",
    "            worksheet = writer.sheets['Sheet1']\n",
    "            for column in df:\n",
    "                column_length = max(df[column].astype(str).map(len).max(), len(column))\n",
    "                col_idx = df.columns.get_loc(column)\n",
    "                col_letter = chr(65 + col_idx)\n",
    "                worksheet.column_dimensions[col_letter].width = column_length + 2\n",
    "\n",
    "# ... (Excel saving code) ...\n",
    "\n",
    "        print(f\"Excel file saved to: {output_file}\")\n",
    "        \n",
    "        # <--- CHANGE 3: The call happens here, after the file exists\n",
    "        true_results_path = 'True_Results.xlsx' \n",
    "        \n",
    "        if os.path.exists(true_results_path):\n",
    "            calculate_pipeline_accuracy(true_results_path, output_file)\n",
    "        else:\n",
    "            print(f\"Skipping accuracy check: '{true_results_path}' not found.\")\n",
    "\n",
    "# Run it\n",
    "main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
