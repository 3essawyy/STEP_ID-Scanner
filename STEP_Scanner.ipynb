{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff14649",
   "metadata": {},
   "source": [
    "# The Following 2 Blocks are scripts to synthesize english number datasets the first is an optimal script while the second is nonoptimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "2cb20fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# OUTPUT_DIR = \"dataset\"\n",
    "# SAMPLES_PER_DIGIT = 200 # How many images per number to generate\n",
    "# IMG_SIZE = (64, 64)      # Canvas size (larger than target to allow rotation/cropping)\n",
    "# FONT_PATH = \"arial.ttf\"  # <--- COPY A FONT FILE HERE!\n",
    "# FONT_SIZE = 45\n",
    "\n",
    "# # Create folders 0-9\n",
    "# if not os.path.exists(OUTPUT_DIR):\n",
    "#     os.makedirs(OUTPUT_DIR)\n",
    "# for i in range(10):\n",
    "#     os.makedirs(os.path.join(OUTPUT_DIR, str(i)), exist_ok=True)\n",
    "\n",
    "# def apply_augmentations(img_pil):\n",
    "#     \"\"\"\n",
    "#     Takes a clean PIL image and ruins it to look like a scanned ID.\n",
    "#     \"\"\"\n",
    "#     # Convert to NumPy for OpenCV processing\n",
    "#     img = np.array(img_pil) \n",
    "    \n",
    "#     # 1. Random Rotation (-10 to 10 degrees)\n",
    "#     angle = random.uniform(-10, 10)\n",
    "#     h, w = img.shape\n",
    "#     M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n",
    "#     img = cv2.warpAffine(img, M, (w, h), borderValue=0) # Black border\n",
    "\n",
    "#     # 2. Gaussian Blur (Simulate out-of-focus camera)\n",
    "#     if random.random() > 0.5:\n",
    "#         k = random.choice([3, 5])\n",
    "#         img = cv2.GaussianBlur(img, (k, k), 0)\n",
    "\n",
    "#     # 3. Noise (Simulate sensor grain)\n",
    "#     noise = np.random.randint(0, 50, (h, w), dtype='uint8')\n",
    "#     # Add noise only to non-black areas mostly, or just add it overall\n",
    "#     img = cv2.add(img, noise)\n",
    "\n",
    "#     # 4. Erosion/Dilation (Simulate ink bleeding or thin print)\n",
    "#     if random.random() > 0.5:\n",
    "#         kernel = np.ones((2,2), np.uint8)\n",
    "#         if random.choice([True, False]):\n",
    "#             img = cv2.erode(img, kernel, iterations=1)\n",
    "#         else:\n",
    "#             img = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "#     return img\n",
    "\n",
    "# def generate_dataset():\n",
    "#     print(f\"Generating {SAMPLES_PER_DIGIT} images per digit...\")\n",
    "    \n",
    "#     try:\n",
    "#         font = ImageFont.truetype(FONT_PATH, FONT_SIZE)\n",
    "#     except IOError:\n",
    "#         print(\"ERROR: Font file not found! Please put 'arial.ttf' in this folder.\")\n",
    "#         return\n",
    "\n",
    "#     for digit in range(10):\n",
    "#         print(f\"Processing digit: {digit}\")\n",
    "#         for i in range(SAMPLES_PER_DIGIT):\n",
    "#             # 1. Create a blank black image\n",
    "#             img_pil = Image.new('L', IMG_SIZE, color=0)\n",
    "#             draw = ImageDraw.Draw(img_pil)\n",
    "            \n",
    "#             # 2. Draw the digit in white centered(ish)\n",
    "#             # We add random offset so the number isn't always perfectly in the middle\n",
    "#             text = str(digit)\n",
    "#             # Get text bounding box to center it\n",
    "#             bbox = draw.textbbox((0, 0), text, font=font)\n",
    "#             text_w = bbox[2] - bbox[0]\n",
    "#             text_h = bbox[3] - bbox[1]\n",
    "            \n",
    "#             x = (IMG_SIZE[0] - text_w) / 2 + random.randint(-5, 5)\n",
    "#             y = (IMG_SIZE[1] - text_h) / 2 + random.randint(-5, 5)\n",
    "            \n",
    "#             draw.text((x, y), text, font=font, fill=255)\n",
    "            \n",
    "#             # 3. Apply the \"Reality\" effects\n",
    "#             final_img = apply_augmentations(img_pil)\n",
    "            \n",
    "#             # 4. Save\n",
    "#             save_path = os.path.join(OUTPUT_DIR, str(digit), f\"{digit}_{i}.png\")\n",
    "#             cv2.imwrite(save_path, final_img)\n",
    "\n",
    "#     print(\"Done! You now have a dataset.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "1ec27cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# OUTPUT_DIR = \"dataset_mixed\"\n",
    "# SAMPLES_PER_DIGIT = 400  # Total images per digit\n",
    "# IMG_SIZE = (64, 64)\n",
    "\n",
    "# # List your fonts here. Ensure these files are in the folder!\n",
    "# FONTS = [\"arial.ttf\", \"Sitka.ttc\"] \n",
    "# FONT_SIZE = 45\n",
    "\n",
    "# # Create output folders\n",
    "# if not os.path.exists(OUTPUT_DIR):\n",
    "#     os.makedirs(OUTPUT_DIR)\n",
    "# for i in range(10):\n",
    "#     os.makedirs(os.path.join(OUTPUT_DIR, str(i)), exist_ok=True)\n",
    "\n",
    "# def add_shadow(img):\n",
    "#     \"\"\"Simulates uneven lighting.\"\"\"\n",
    "#     h, w = img.shape\n",
    "#     top_left = random.uniform(0.5, 1.0)\n",
    "#     bot_right = random.uniform(0.5, 1.0)\n",
    "#     X, Y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "#     mask = top_left + (bot_right - top_left) * (X / w)\n",
    "#     img = img.astype('float32') * mask\n",
    "#     return img.astype('uint8')\n",
    "\n",
    "# def break_character(img):\n",
    "#     \"\"\"Simulates scratches and missing chunks.\"\"\"\n",
    "#     h, w = img.shape\n",
    "    \n",
    "#     # Random scratch line\n",
    "#     if random.random() > 0.5:\n",
    "#         num_scratches = random.randint(1, 3)\n",
    "#         for _ in range(num_scratches):\n",
    "#             x1, y1 = random.randint(0, w), random.randint(0, h)\n",
    "#             x2, y2 = random.randint(0, w), random.randint(0, h)\n",
    "#             cv2.line(img, (x1, y1), (x2, y2), 0, random.randint(1, 3))\n",
    "\n",
    "#     # Random noise chunks missing\n",
    "#     if random.random() > 0.5:\n",
    "#         noise = np.zeros((h, w), dtype='uint8')\n",
    "#         cv2.randn(noise, 0, 255)\n",
    "#         _, holes = cv2.threshold(noise, 200, 255, cv2.THRESH_BINARY)\n",
    "#         img = cv2.subtract(img, holes)\n",
    "        \n",
    "#     return img\n",
    "\n",
    "# def apply_defects(img_pil):\n",
    "#     img = np.array(img_pil)\n",
    "    \n",
    "#     # 1. Perspective Warp\n",
    "#     h, w = img.shape\n",
    "#     src_points = np.float32([[0,0], [w,0], [0,h], [w,h]])\n",
    "#     dst_points = np.float32([\n",
    "#         [random.randint(0, 5), random.randint(0, 5)],\n",
    "#         [w - random.randint(0, 5), random.randint(0, 5)],\n",
    "#         [random.randint(0, 5), h - random.randint(0, 5)],\n",
    "#         [w - random.randint(0, 5), h - random.randint(0, 5)]\n",
    "#     ])\n",
    "#     M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "#     img = cv2.warpPerspective(img, M, (w, h))\n",
    "\n",
    "#     # 2. Break Character\n",
    "#     img = break_character(img)\n",
    "\n",
    "#     # 3. Blur\n",
    "#     if random.random() > 0.3:\n",
    "#         k = random.choice([3, 5])\n",
    "#         img = cv2.GaussianBlur(img, (k, k), 0)\n",
    "\n",
    "#     # 4. Shadow\n",
    "#     if random.random() > 0.4:\n",
    "#         img = add_shadow(img)\n",
    "\n",
    "#     # 5. Salt & Pepper Noise\n",
    "#     noise_prob = 0.02\n",
    "#     thres = 1 - noise_prob\n",
    "#     rdn = np.random.random(img.shape)\n",
    "#     img[rdn < noise_prob] = 0\n",
    "#     img[rdn > thres] = 255\n",
    "\n",
    "#     return img\n",
    "\n",
    "# def generate_dataset():\n",
    "#     print(f\"Generating {SAMPLES_PER_DIGIT} images per digit using fonts: {FONTS}\")\n",
    "    \n",
    "#     # Verify fonts exist\n",
    "#     loaded_fonts = []\n",
    "#     for f_name in FONTS:\n",
    "#         try:\n",
    "#             loaded_fonts.append(ImageFont.truetype(f_name, FONT_SIZE))\n",
    "#         except IOError:\n",
    "#             print(f\"WARNING: Could not load {f_name}. Skipping.\")\n",
    "    \n",
    "#     if not loaded_fonts:\n",
    "#         print(\"ERROR: No fonts found! Please copy .ttf files to this folder.\")\n",
    "#         return\n",
    "\n",
    "#     for digit in range(10):\n",
    "#         print(f\"Processing digit: {digit}\")\n",
    "#         for i in range(SAMPLES_PER_DIGIT):\n",
    "#             # 1. Randomly choose a font\n",
    "#             font = random.choice(loaded_fonts)\n",
    "            \n",
    "#             # 2. Draw Text\n",
    "#             img_pil = Image.new('L', IMG_SIZE, color=0)\n",
    "#             draw = ImageDraw.Draw(img_pil)\n",
    "            \n",
    "#             text = str(digit)\n",
    "#             bbox = draw.textbbox((0, 0), text, font=font)\n",
    "#             text_w, text_h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "#             x = (IMG_SIZE[0] - text_w) / 2 + random.randint(-8, 8)\n",
    "#             y = (IMG_SIZE[1] - text_h) / 2 + random.randint(-8, 8)\n",
    "            \n",
    "#             draw.text((x, y), text, font=font, fill=255)\n",
    "            \n",
    "#             # 3. Apply Defects\n",
    "#             final_img = apply_defects(img_pil)\n",
    "            \n",
    "#             # 4. Save\n",
    "#             save_path = os.path.join(OUTPUT_DIR, str(digit), f\"{digit}_{i}.png\")\n",
    "#             cv2.imwrite(save_path, final_img)\n",
    "\n",
    "#     print(f\"Done! Created {SAMPLES_PER_DIGIT * 10} images in '{OUTPUT_DIR}'\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "6c180f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import pytesseract\n",
    "from sklearn.svm import LinearSVC\n",
    "# Depending on library versions on your system, one of the following imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe' # (Windows Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "c32ccb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"./Synthetic_numbers_dataset_mixed\"\n",
    "\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "3a33c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    You won't implement anything in this function. You just need to understand it \n",
    "    and understand its parameters (i.e win_size, cell_size, ... etc)\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "1e45a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(feature_set='hog'):\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames = os.listdir(path_to_dataset)\n",
    "    total_processed = 0\n",
    "    total_images = 0\n",
    "    for folder in img_filenames:    \n",
    "        total_images += len(img_filenames) * len(os.listdir(os.path.join(path_to_dataset, folder)))   # 10 folders * 400 images each = 4000\n",
    "    total_images = total_images // 10\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "        NumberInnerPath = os.path.join(path_to_dataset, str(i))\n",
    "        NumberInnerPath = os.listdir(NumberInnerPath)\n",
    "        for idx, img_filename in enumerate(NumberInnerPath):\n",
    "            if img_filename.split('.')[-1] != 'png':\n",
    "                continue\n",
    "\n",
    "            label = str(i)\n",
    "            labels.append(label)\n",
    "\n",
    "            # Build the full file path\n",
    "            full_path = os.path.join(path_to_dataset, str(i), img_filename)\n",
    "            img = cv2.imread(full_path)\n",
    "            features.append(extract_hog_features(img))\n",
    "            \n",
    "            total_processed += 1\n",
    "            \n",
    "            # show an update every 50 images\n",
    "            if total_processed > 0 and total_processed % 50 == 0:\n",
    "                print(\"[INFO] processed {}/{} images\".format(total_processed, total_images))\n",
    "        \n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "5092b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVM():\n",
    "    SVMclassifier=svm.LinearSVC(random_state=random_seed)\n",
    "    features, labels = load_dataset()\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.4, random_state=random_seed)\n",
    "    SVMclassifier.fit(train_features,train_labels)\n",
    "    accuracy=SVMclassifier.score(test_features,test_labels)\n",
    "    print(\"SVM Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "cf82cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 50/4000 images\n",
      "[INFO] processed 100/4000 images\n",
      "[INFO] processed 150/4000 images\n",
      "[INFO] processed 200/4000 images\n",
      "[INFO] processed 250/4000 images\n",
      "[INFO] processed 300/4000 images\n",
      "[INFO] processed 350/4000 images\n",
      "[INFO] processed 400/4000 images\n",
      "[INFO] processed 450/4000 images\n",
      "[INFO] processed 500/4000 images\n",
      "[INFO] processed 550/4000 images\n",
      "[INFO] processed 600/4000 images\n",
      "[INFO] processed 650/4000 images\n",
      "[INFO] processed 700/4000 images\n",
      "[INFO] processed 750/4000 images\n",
      "[INFO] processed 800/4000 images\n",
      "[INFO] processed 850/4000 images\n",
      "[INFO] processed 900/4000 images\n",
      "[INFO] processed 950/4000 images\n",
      "[INFO] processed 1000/4000 images\n",
      "[INFO] processed 1050/4000 images\n",
      "[INFO] processed 1100/4000 images\n",
      "[INFO] processed 1150/4000 images\n",
      "[INFO] processed 1200/4000 images\n",
      "[INFO] processed 1250/4000 images\n",
      "[INFO] processed 1300/4000 images\n",
      "[INFO] processed 1350/4000 images\n",
      "[INFO] processed 1400/4000 images\n",
      "[INFO] processed 1450/4000 images\n",
      "[INFO] processed 1500/4000 images\n",
      "[INFO] processed 1550/4000 images\n",
      "[INFO] processed 1600/4000 images\n",
      "[INFO] processed 1650/4000 images\n",
      "[INFO] processed 1700/4000 images\n",
      "[INFO] processed 1750/4000 images\n",
      "[INFO] processed 1800/4000 images\n",
      "[INFO] processed 1850/4000 images\n",
      "[INFO] processed 1900/4000 images\n",
      "[INFO] processed 1950/4000 images\n",
      "[INFO] processed 2000/4000 images\n",
      "[INFO] processed 2050/4000 images\n",
      "[INFO] processed 2100/4000 images\n",
      "[INFO] processed 2150/4000 images\n",
      "[INFO] processed 2200/4000 images\n",
      "[INFO] processed 2250/4000 images\n",
      "[INFO] processed 2300/4000 images\n",
      "[INFO] processed 2350/4000 images\n",
      "[INFO] processed 2400/4000 images\n",
      "[INFO] processed 2450/4000 images\n",
      "[INFO] processed 2500/4000 images\n",
      "[INFO] processed 2550/4000 images\n",
      "[INFO] processed 2600/4000 images\n",
      "[INFO] processed 2650/4000 images\n",
      "[INFO] processed 2700/4000 images\n",
      "[INFO] processed 2750/4000 images\n",
      "[INFO] processed 2800/4000 images\n",
      "[INFO] processed 2850/4000 images\n",
      "[INFO] processed 2900/4000 images\n",
      "[INFO] processed 2950/4000 images\n",
      "[INFO] processed 3000/4000 images\n",
      "[INFO] processed 3050/4000 images\n",
      "[INFO] processed 3100/4000 images\n",
      "[INFO] processed 3150/4000 images\n",
      "[INFO] processed 3200/4000 images\n",
      "[INFO] processed 3250/4000 images\n",
      "[INFO] processed 3300/4000 images\n",
      "[INFO] processed 3350/4000 images\n",
      "[INFO] processed 3400/4000 images\n",
      "[INFO] processed 3450/4000 images\n",
      "[INFO] processed 3500/4000 images\n",
      "[INFO] processed 3550/4000 images\n",
      "[INFO] processed 3600/4000 images\n",
      "[INFO] processed 3650/4000 images\n",
      "[INFO] processed 3700/4000 images\n",
      "[INFO] processed 3750/4000 images\n",
      "[INFO] processed 3800/4000 images\n",
      "[INFO] processed 3850/4000 images\n",
      "[INFO] processed 3900/4000 images\n",
      "[INFO] processed 3950/4000 images\n",
      "[INFO] processed 4000/4000 images\n",
      "SVM Accuracy:  0.875625\n"
     ]
    }
   ],
   "source": [
    "train_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "e532a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images in: test_arabic_names_full...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Extracted Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0.png</td>\n",
       "      <td>محمود محمد يوسف السيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_10.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_11.png</td>\n",
       "      <td>عبدالله مصطفى إبراهيم الشناوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_12.png</td>\n",
       "      <td>أحمد خالد عبدالله الشناوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id_13.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id_14.png</td>\n",
       "      <td>زياد مصطفى حسين سليمان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id_15.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id_16.png</td>\n",
       "      <td>حسين خالد حسين عبدالرحمن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id_17.png</td>\n",
       "      <td>سعيد يوسف خالد سليمان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>id_18.png</td>\n",
       "      <td>أحمد خالد أحمد سليمان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>id_19.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>id_2.png</td>\n",
       "      <td>علي طارق أحمد المنشاوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>id_20.png</td>\n",
       "      <td>عمر إبراهيم كريم نجيب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id_21.png</td>\n",
       "      <td>زياد حسن يوسف المصري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id_22.png</td>\n",
       "      <td>خالد يوسف طارق نجيب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>id_23.png</td>\n",
       "      <td>مصطفى خالد حسن عزت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>id_24.png</td>\n",
       "      <td>كريم علي أحمد الشناوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>id_25.png</td>\n",
       "      <td>حسن حسين طارق الشناوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>id_26.png</td>\n",
       "      <td>سعيد سعيد زياد عزت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>id_27.png</td>\n",
       "      <td>محمد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>id_28.png</td>\n",
       "      <td>يوسف عمر محمود عامر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>id_29.png</td>\n",
       "      <td>طارق حسين زياد المنشاوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>id_3.png</td>\n",
       "      <td>علي إبراهيم خالد عزت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>id_30.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>id_31.png</td>\n",
       "      <td>إبراهيم حسن مصطفى المصري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>id_32.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>id_33.png</td>\n",
       "      <td>محمود مصطفى عبدالله السيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>id_34.png</td>\n",
       "      <td>أحمد خالد علي الشناوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>id_35.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>id_36.png</td>\n",
       "      <td>كريم حسين يوسف عزت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>id_37.png</td>\n",
       "      <td>طارق إبراهيم إبراهيم عامر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>id_38.png</td>\n",
       "      <td>سعيد مصطفى زياد عزت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>id_39.png</td>\n",
       "      <td>زياد محمود خالد السيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>id_4.png</td>\n",
       "      <td>محمد إبراهيم طارق السيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>id_40.png</td>\n",
       "      <td>عزت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>id_41.png</td>\n",
       "      <td>خالد خالد محمد الشناوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>id_42.png</td>\n",
       "      <td>أحمد خالد علي المنشاوي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>id_43.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>id_44.png</td>\n",
       "      <td>كريم إبراهيم حسن عبدالرحمن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>id_45.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>id_46.png</td>\n",
       "      <td>محمود محمود طارق سليمان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>id_47.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>id_48.png</td>\n",
       "      <td>محمود أحمد سعيد سليمان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>id_49.png</td>\n",
       "      <td>محمود خالد إبراهيم السيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>id_5.png</td>\n",
       "      <td>زياد يوسف محمد المصري</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>id_6.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>id_7.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>id_8.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>id_9.png</td>\n",
       "      <td>يوسف أحمد زياد عزت</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Filename                 Extracted Name\n",
       "0    id_0.png          محمود محمد يوسف السيد\n",
       "1    id_1.png                               \n",
       "2   id_10.png                               \n",
       "3   id_11.png  عبدالله مصطفى إبراهيم الشناوي\n",
       "4   id_12.png      أحمد خالد عبدالله الشناوي\n",
       "5   id_13.png                               \n",
       "6   id_14.png         زياد مصطفى حسين سليمان\n",
       "7   id_15.png                               \n",
       "8   id_16.png       حسين خالد حسين عبدالرحمن\n",
       "9   id_17.png          سعيد يوسف خالد سليمان\n",
       "10  id_18.png          أحمد خالد أحمد سليمان\n",
       "11  id_19.png                               \n",
       "12   id_2.png         علي طارق أحمد المنشاوي\n",
       "13  id_20.png          عمر إبراهيم كريم نجيب\n",
       "14  id_21.png           زياد حسن يوسف المصري\n",
       "15  id_22.png            خالد يوسف طارق نجيب\n",
       "16  id_23.png             مصطفى خالد حسن عزت\n",
       "17  id_24.png          كريم علي أحمد الشناوي\n",
       "18  id_25.png          حسن حسين طارق الشناوي\n",
       "19  id_26.png             سعيد سعيد زياد عزت\n",
       "20  id_27.png                           محمد\n",
       "21  id_28.png            يوسف عمر محمود عامر\n",
       "22  id_29.png        طارق حسين زياد المنشاوي\n",
       "23   id_3.png           علي إبراهيم خالد عزت\n",
       "24  id_30.png                               \n",
       "25  id_31.png       إبراهيم حسن مصطفى المصري\n",
       "26  id_32.png                               \n",
       "27  id_33.png      محمود مصطفى عبدالله السيد\n",
       "28  id_34.png          أحمد خالد علي الشناوي\n",
       "29  id_35.png                               \n",
       "30  id_36.png             كريم حسين يوسف عزت\n",
       "31  id_37.png      طارق إبراهيم إبراهيم عامر\n",
       "32  id_38.png            سعيد مصطفى زياد عزت\n",
       "33  id_39.png          زياد محمود خالد السيد\n",
       "34   id_4.png        محمد إبراهيم طارق السيد\n",
       "35  id_40.png                            عزت\n",
       "36  id_41.png         خالد خالد محمد الشناوي\n",
       "37  id_42.png         أحمد خالد علي المنشاوي\n",
       "38  id_43.png                               \n",
       "39  id_44.png     كريم إبراهيم حسن عبدالرحمن\n",
       "40  id_45.png                               \n",
       "41  id_46.png        محمود محمود طارق سليمان\n",
       "42  id_47.png                               \n",
       "43  id_48.png         محمود أحمد سعيد سليمان\n",
       "44  id_49.png       محمود خالد إبراهيم السيد\n",
       "45   id_5.png          زياد يوسف محمد المصري\n",
       "46   id_6.png                               \n",
       "47   id_7.png                               \n",
       "48   id_8.png                               \n",
       "49   id_9.png             يوسف أحمد زياد عزت"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extractname(img_path):\n",
    "    \n",
    "    # --- HELPER: TEXT CLEANER ---\n",
    "    def clean_text(raw_text):\n",
    "        if not raw_text: return \"\"\n",
    "        # Keep Arabic letters (0621-064A) and spaces\n",
    "        cleaned = re.sub(r'[^\\u0621-\\u064A\\s]', '', raw_text)\n",
    "        cleaned = cleaned.replace('\\n', ' ')\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        return cleaned\n",
    "\n",
    "    # --- LOAD IMAGE ---\n",
    "    # We load the image INSIDE the function now to handle transparency\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None: return \"\"\n",
    "\n",
    "    # Fix Transparency (PNG)\n",
    "    if img.shape[2] == 4:\n",
    "        alpha = img[:, :, 3]\n",
    "        rgb = img[:, :, :3]\n",
    "        white_bg = np.ones_like(rgb, dtype=np.uint8) * 255\n",
    "        alpha_factor = alpha[:, :, np.newaxis] / 255.0\n",
    "        img = (rgb * alpha_factor + white_bg * (1 - alpha_factor)).astype(np.uint8)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    \n",
    "    configs = [\n",
    "        # --- TIER 1: The Safe Zone ---\n",
    "        {'mode': 'rgb', 'scale': 1, 'process': 'none', 'psm': 7},\n",
    "        {'mode': 'gray', 'scale': 2, 'process': 'none', 'psm': 6},\n",
    "\n",
    "        # --- TIER 2: The Standard Zone ---\n",
    "        {'mode': 'gray', 'scale': 2, 'process': 'otsu', 'psm': 7},\n",
    "        {'mode': 'gray', 'scale': 2, 'process': 'blur_otsu', 'psm': 6},\n",
    "\n",
    "        # --- TIER 3: The \"Last Resort\" ---\n",
    "        {'mode': 'gray', 'scale': 2, 'process': 'normalize_otsu', 'psm': 6},\n",
    "        {'mode': 'gray', 'scale': 2, 'process': 'dilate', 'psm': 6},\n",
    "        {'mode': 'gray', 'scale': 2, 'process': 'morph_open', 'psm': 6}\n",
    "    ]\n",
    "\n",
    "    for config in configs:\n",
    "        # A. Source Selection\n",
    "        if config['mode'] == 'rgb':\n",
    "            current = img_rgb.copy()\n",
    "        else:\n",
    "            current = img_gray.copy()\n",
    "            \n",
    "        # B. Scaling\n",
    "        if config['scale'] > 1:\n",
    "            h, w = current.shape[:2]\n",
    "            current = cv2.resize(current, (w * config['scale'], h * config['scale']), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "        # C. Processing Logic\n",
    "        process = config['process']\n",
    "        \n",
    "        if process == 'otsu':\n",
    "            _, current = cv2.threshold(current, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "        elif process == 'blur_otsu':\n",
    "            current = cv2.GaussianBlur(current, (5, 5), 0)\n",
    "            _, current = cv2.threshold(current, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "        elif process == 'normalize_otsu':\n",
    "            current = cv2.normalize(current, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            _, current = cv2.threshold(current, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "        elif process == 'dilate':\n",
    "            _, binary = cv2.threshold(current, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            inverted = cv2.bitwise_not(binary)\n",
    "            kernel = np.ones((2,2), np.uint8)\n",
    "            dilated = cv2.dilate(inverted, kernel, iterations=1)\n",
    "            current = cv2.bitwise_not(dilated)\n",
    "            \n",
    "        elif process == 'morph_open':\n",
    "            _, binary = cv2.threshold(current, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            kernel = np.ones((2,2), np.uint8)\n",
    "            current = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # D. Padding\n",
    "        current = cv2.copyMakeBorder(current, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "        \n",
    "        # E. Execution\n",
    "        custom_config = f\"--oem 3 --psm {config['psm']}\"\n",
    "        text = pytesseract.image_to_string(current, lang='ara', config=custom_config)\n",
    "        final_text = clean_text(text)\n",
    "        \n",
    "        # F. Validation\n",
    "        if len(final_text) > 2:\n",
    "            return final_text\n",
    "\n",
    "    return \"\"\n",
    "folder_path = 'test_arabic_names_full'\n",
    "data = []\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"Processing images in: {folder_path}...\\n\")\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is an image\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            extracted_text = extractname(full_path)    \n",
    "            clean_text = extracted_text.strip()\n",
    "            data.append({'Filename': filename, 'Extracted Name': clean_text})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    display(df.head(50))\n",
    "    \n",
    "else:\n",
    "    print(f\"the folder '{folder_path}' was not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "b8dbd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved to 'arabic_names_final.xlsx' with auto-adjusted columns.\n"
     ]
    }
   ],
   "source": [
    "if 'df' in locals() and not df.empty:\n",
    "    final_df = df.copy()\n",
    "    final_df['ID'] = final_df.apply(lambda x: random.randint(1000000, 9999999), axis=1)\n",
    "        # Rename and Select Columns\n",
    "    final_df.rename(columns={'Extracted Name': 'Name'}, inplace=True)\n",
    "    final_df = final_df[['Name', 'ID']]    \n",
    "    output_filename = 'arabic_names_final.xlsx'\n",
    "    with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "        final_df.to_excel(writer, sheet_name='Names', index=False)\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Names']\n",
    "        for i, column in enumerate(final_df.columns):\n",
    "            column_letter = get_column_letter(i + 1) # Get A, B, C...\n",
    "            max_len = final_df[column].astype(str).map(len).max()\n",
    "            max_len = max(max_len, len(column))\n",
    "            worksheet.column_dimensions[column_letter].width = (max_len + 2) * 1.2\n",
    "            \n",
    "    print(f\"Success! Saved to '{output_filename}' with auto-adjusted columns.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Please run the OCR extraction code first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
