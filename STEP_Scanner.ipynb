{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba08ac",
   "metadata": {},
   "source": [
    "## **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10cf11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pytesseract\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "from openpyxl.utils import get_column_letter\n",
    "from commonfunctions import *\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' \n",
    "\n",
    "\n",
    "target_img_size = (32, 32) \n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f94a9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Single-ID preprocessing debug (set RAW_ID and run) ===\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "# RAW_ID = 7  # <- change this (e.g., 2, 14, 37...)\n",
    "# REFERENCE_ID = 14  # reference card used for alignment\n",
    "# EXT = '.jpg'\n",
    "# APPLY_GLARE_REMOVAL = True  # <- set False to skip\n",
    "# GLARE_V_THRESH = 235\n",
    "# GLARE_S_THRESH = 80\n",
    "# APPLY_PIL_UPSCALE = True  # <- set False to load with cv2.imread only\n",
    "\n",
    "\n",
    "# def enhance_image_quality(image_path):\n",
    "#     \"\"\"Enhance image quality to improve OCR accuracy.\"\"\"\n",
    "#     try:\n",
    "#         img = Image.open(image_path)\n",
    "#         # Increase resolution if image is small\n",
    "#         if max(img.size) < 800:\n",
    "#             new_size = (img.size[0] * 2, img.size[1] * 2)\n",
    "#             img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "#         return img\n",
    "#     except Exception:\n",
    "#         return Image.open(image_path)\n",
    "\n",
    "\n",
    "# def load_image_bgr(image_path: str) -> np.ndarray:\n",
    "#     \"\"\"Load image as OpenCV BGR, optionally upscaling with PIL first.\"\"\"\n",
    "#     if APPLY_PIL_UPSCALE:\n",
    "#         try:\n",
    "#             pil_img = enhance_image_quality(image_path).convert('RGB')\n",
    "#             rgb = np.array(pil_img)\n",
    "#             return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     return cv2.imread(image_path)\n",
    "\n",
    "\n",
    "# def _show(img, title):\n",
    "#     if img is None:\n",
    "#         print(f\"{title}: <None>\")\n",
    "#         return\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     if len(img.shape) == 2:\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#     else:\n",
    "#         plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# base_dir = os.getcwd()\n",
    "# raw_path = os.path.join(base_dir, 'Raw_IDs', f'ID{RAW_ID}{EXT}')\n",
    "# ref_path = os.path.join(base_dir, 'Raw_IDs', f'ID{REFERENCE_ID}{EXT}')\n",
    "\n",
    "# if not os.path.exists(raw_path):\n",
    "#     raise FileNotFoundError(f\"Raw image not found: {raw_path}\")\n",
    "# if not os.path.exists(ref_path):\n",
    "#     raise FileNotFoundError(f\"Reference image not found: {ref_path}\")\n",
    "\n",
    "# raw_img = load_image_bgr(raw_path)\n",
    "# _show(raw_img, f\"Step 0 - Raw (ID{RAW_ID})\")\n",
    "\n",
    "# if APPLY_GLARE_REMOVAL:\n",
    "#     glare_fixed, glare_mask = remove_glare_spots(raw_img, v_thresh=GLARE_V_THRESH, s_thresh=GLARE_S_THRESH)\n",
    "#     _show(glare_mask, \"Step 0b - Glare mask\")\n",
    "#     _show(glare_fixed, \"Step 0c - After glare removal\")\n",
    "#     img0 = glare_fixed\n",
    "# else:\n",
    "#     img0 = raw_img\n",
    "\n",
    "# clean_img, is_impulsive = is_impulsive_noise(img0)\n",
    "# print('Impulsive noise detected:', is_impulsive)\n",
    "# _show(clean_img, \"Step 1 - After impulsive noise removal\")\n",
    "\n",
    "# aligned_img = align_images_sift(clean_img, ref_path)\n",
    "# _show(aligned_img, f\"Step 2 - After alignment to ID{REFERENCE_ID}\")\n",
    "\n",
    "# clean_img2, is_random = is_random_noise(aligned_img)\n",
    "# print('Random noise detected:', is_random)\n",
    "# _show(clean_img2, \"Step 3 - After random noise removal\")\n",
    "\n",
    "# enhanced_img = enhance_contrast_clahe(clean_img2)\n",
    "# _show(enhanced_img, \"Step 4 - After contrast enhancement\")\n",
    "\n",
    "# name_img, digit_imgs, daf3_digits = extract_name_and_digits(enhanced_img)\n",
    "# _show(name_img, \"Step 5a - Cropped name region\")\n",
    "\n",
    "# # Show code digits (first row)\n",
    "# if isinstance(digit_imgs, (list, tuple)) and len(digit_imgs) > 0:\n",
    "#     for idx, d in enumerate(digit_imgs):\n",
    "#         _show(d, f\"Step 5b - Code digit {idx}\")\n",
    "# else:\n",
    "#     print('No code digits returned from extract_name_and_digits')\n",
    "\n",
    "# # Show Daf3 digits\n",
    "# if isinstance(daf3_digits, (list, tuple)) and len(daf3_digits) > 0:\n",
    "#     for idx, d in enumerate(daf3_digits):\n",
    "#         _show(d, f\"Step 5c - Daf3 digit {idx}\")\n",
    "# else:\n",
    "#     print('No Daf3 digits returned from extract_name_and_digits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cc75f",
   "metadata": {},
   "source": [
    "## GPU setup (EasyOCR / Qwen / PaddleOCR)\n",
    "Run the next cell to check if CUDA is available in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab16d",
   "metadata": {},
   "source": [
    "## **Main Functions Overview**\n",
    "\n",
    "- **Image Alignment**\n",
    "  - Detects SIFT keypoints and descriptors in the input and reference images.\n",
    "  - Matches them using the ratio test.\n",
    "  - Uses RANSAC to estimate a homography.\n",
    "  - Applies the homography to warp the input image so it lines up with the reference.\n",
    "  - Returns the aligned image (or the original if not enough matches are found).\n",
    "\n",
    "- **Extract Details**\n",
    "  - Uses (x, y, w, h) coordinates to crop the aligned card into:\n",
    "    - The name region\n",
    "    - The code (ID) region\n",
    "  - Returns these sub-images for downstream OCR or digit processing.\n",
    "\n",
    "- **Save Student Name**\n",
    "  - Ensures the output folder exists.\n",
    "  - Writes the cropped name image to disk with a filename that includes the student ID.\n",
    "  - Creates a persistent record usable for manual review or OCR.\n",
    "\n",
    "- **Split and Save Digits**\n",
    "  - Converts the code region to grayscale and applies Otsu thresholding.\n",
    "  - Finds contours and filters out small noise.\n",
    "  - Selects the largest seven contours (by area) and sorts them left-to-right.\n",
    "  - Saves each detected digit crop into a per-student folder as individual image files.\n",
    "\n",
    "- **save_split_digits**\n",
    "  - Takes a list of digit images for a student.\n",
    "  - Ensures a folder exists for each student (named by their ID).\n",
    "  - Saves each digit image as `digit_0.jpg`, `digit_1.jpg`, ..., `digit_6.jpg` inside the student’s folder.\n",
    "  - Used for batch saving when all digit crops are already extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945bafe",
   "metadata": {},
   "source": [
    "## **Noise Detection and Treatment**\n",
    "- **Impulsive Noise (Median Filter)**\n",
    "- **Random Noise (Gaussian Filter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f54ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_random_noise(img, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detects random noise and applies Non-Local Means (NLM) Denoising.\n",
    "    This is the best traditional filter for preserving textures and sharp edges.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    stddev = np.std(img)\n",
    "    normalized_stddev = stddev / 255.0\n",
    "\n",
    "    if normalized_stddev < threshold:\n",
    "        return img, False\n",
    "\n",
    "    treated_img = cv2.fastNlMeansDenoising(\n",
    "        img, \n",
    "        None, \n",
    "        h=10, \n",
    "        templateWindowSize=7, \n",
    "        searchWindowSize=21\n",
    "    )\n",
    "    \n",
    "    return treated_img, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53737cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_impulsive_noise(img, threshold=0.1, black_range=(0, 9), white_range=(246, 255)):\n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    total_pixels = img.size\n",
    "    is_pepper = (img >= black_range[0]) & (img <= black_range[1])\n",
    "    is_salt = (img >= white_range[0]) & (img <= white_range[1])\n",
    "\n",
    "    noise_mask = is_pepper | is_salt\n",
    "    num_noise_pixels = np.sum(noise_mask)\n",
    "    prop = num_noise_pixels / total_pixels\n",
    "\n",
    "    if prop < threshold:\n",
    "        return img, False \n",
    "\n",
    "    k = int(3 + prop * 10)\n",
    "    if k % 2 == 0: k += 1\n",
    "    k = min(max(k, 3), 9)\n",
    "\n",
    "    median_filtered = cv2.medianBlur(img, k)\n",
    "\n",
    "    treated_img = img.copy()\n",
    "\n",
    "    treated_img[noise_mask] = median_filtered[noise_mask]\n",
    "\n",
    "    return treated_img, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba8fd2",
   "metadata": {},
   "source": [
    "Contrast enhancment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0082918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def enhance_contrast_clahe(img, clip_limit=2.0, tile_size=(8, 8)):\n",
    "\n",
    "    if len(img.shape) == 3:\n",
    "\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Create CLAHE object\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "        cl = clahe.apply(l)\n",
    "        \n",
    "        # Merge back and convert to BGR\n",
    "        enhanced_lab = cv2.merge((cl, a, b))\n",
    "        return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "    else:\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
    "        return clahe.apply(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf03c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images_sift(img_to_align, reference_path):\n",
    "    img1 = img_to_align\n",
    "    img2 = cv2.imread(reference_path) \n",
    "    \n",
    "    # --- FIX: Check if img1 is already grayscale ---\n",
    "    if len(img1.shape) == 3:\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray1 = img1 # Already grayscale\n",
    "\n",
    "    # Ref image is loaded from disk, usually BGR, but good to check\n",
    "    if len(img2.shape) == 3:\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray2 = img2\n",
    "\n",
    "    sift = cv2.SIFT_create() \n",
    "    \n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        h, w = img2.shape[:2]\n",
    "        aligned_img = cv2.warpPerspective(img1, M, (w, h))\n",
    "\n",
    "        return aligned_img\n",
    "    \n",
    "    else:\n",
    "        print(f\"Not enough matches found: {len(good_matches)}/10\")\n",
    "        return img1\n",
    "    \n",
    "\n",
    "\n",
    "def extract_details(aligned_image):\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    \n",
    "    name_contour = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_contour = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    \n",
    "    return name_contour, code_contour\n",
    "\n",
    "\n",
    "def save_student_name(student_id, name_img, output_folder=\"extracted_names\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    # Construct filename: extracted_names/ID1_name.jpg\n",
    "    filename = f\"{output_folder}/{student_id}_name.jpg\"\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(filename, name_img)\n",
    "    \n",
    "\n",
    "def split_and_save_digits(student_id, code_roi, output_folder=\"extracted_digits\"):\n",
    "    save_path = f\"{output_folder}/ID{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    gray = cv2.cvtColor(code_roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # A. Collect all valid candidates\n",
    "    candidates = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        # Filter tiny noise\n",
    "        if h > 15 and w > 5:\n",
    "            candidates.append((x, y, w, h, area))\n",
    "\n",
    "    candidates = sorted(candidates, key=lambda c: c[4], reverse=True) \n",
    "    final_digits = candidates[:7] \n",
    "    \n",
    "    # C. Sort left to right\n",
    "    final_digits = sorted(final_digits, key=lambda c: c[0])\n",
    "        \n",
    "    # D. Save\n",
    "    for index, (x, y, w, h, area) in enumerate(final_digits):\n",
    "        digit_img = code_roi[y:y+h, x:x+w]\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)\n",
    "\n",
    "      \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_name_and_digits(aligned_image):\n",
    "\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    daf3_coords = (350, 500, 620, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    dx, dy, dw, dh = daf3_coords\n",
    "    \n",
    "    # Extract ROIs\n",
    "    name_img = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_roi = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    daf3_img = aligned_image[dy:dy+dh, dx:dx+dw]\n",
    "    \n",
    "    # --- Helper Function to Process Any ROI ---\n",
    "    def process_roi_digits(roi_img, digit_limit):\n",
    "\n",
    "        if len(roi_img.shape) == 3:\n",
    "            gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = roi_img\n",
    "        \n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        candidates = []\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            area = w * h\n",
    "            \n",
    "            if h > 15 and w > 5:\n",
    "                \n",
    "                if w > 0.8 * h: \n",
    "                    half_w = w // 2\n",
    "                    candidates.append((x, y, half_w, h, half_w * h))\n",
    "                    candidates.append((x + half_w, y, half_w, h, half_w * h))\n",
    "                else:\n",
    "                    candidates.append((x, y, w, h, area))\n",
    "        \n",
    "\n",
    "        candidates = sorted(candidates, key=lambda c: c[4], reverse=True)[:digit_limit]\n",
    "\n",
    "        final_candidates = sorted(candidates, key=lambda c: c[0])\n",
    "        \n",
    "\n",
    "        cropped_digits = []\n",
    "        for (x, y, w, h, area) in final_candidates:\n",
    "            digit_crop = roi_img[y:y+h, x:x+w]\n",
    "            cropped_digits.append(digit_crop)\n",
    "            \n",
    "        return cropped_digits\n",
    "    \n",
    "\n",
    "    code_digits = process_roi_digits(code_roi, digit_limit=7)\n",
    "\n",
    "    daf3_digits = process_roi_digits(daf3_img, digit_limit=14)\n",
    "\n",
    "    return name_img, code_digits, daf3_digits\n",
    "\n",
    "def save_split_digits(student_id, digit_imgs, output_folder=\"extracted_digits\"):\n",
    "  \n",
    "    save_path = f\"{output_folder}/{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for index, digit_img in enumerate(digit_imgs):\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f17014",
   "metadata": {},
   "source": [
    "## **SVM English Number Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ff7086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_dataset = r\"train_digits\" \n",
    "\n",
    "def train_SVM_robust():\n",
    "\n",
    "    label_map = {\n",
    "        'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', \n",
    "        'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '9'\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    img_filenames = os.listdir(path_to_train_dataset)\n",
    "    print(f\"Loading {len(img_filenames)} training images...\")\n",
    "\n",
    "    for fn in img_filenames:\n",
    "        if not fn.lower().endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "\n",
    "        prefix = fn[0].lower()\n",
    "        if prefix in label_map:\n",
    "            labels.append(label_map[prefix])\n",
    "            \n",
    "            path = os.path.join(path_to_train_dataset, fn)\n",
    "            img = cv2.imread(path)\n",
    "            \n",
    "            features.append(extract_hog_features(img))\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', LinearSVC(random_state=42, max_iter=5000, dual=False))\n",
    "    ])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(f\"Training Complete. Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32)) \n",
    "    \n",
    "    win_size = (32, 32)\n",
    "    cell_size = (8, 8) \n",
    "    block_size = (16, 16)\n",
    "    block_stride = (8, 8)\n",
    "    nbins = 9\n",
    "    \n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fbfe0",
   "metadata": {},
   "source": [
    "# **Tesseract Arabic OCR**\n",
    "\n",
    "## **Current Situation**\n",
    "\n",
    "The project uses Tesseract OCR to extract Arabic names from scanned images. Initially, the extraction pipeline achieved only a **70% success rate**. This meant that about 30% of the images failed to yield any valid Arabic text, even though the images were visually clear and contained readable names.\n",
    "\n",
    "## **Why Was the Success Rate Only 70%?**\n",
    "\n",
    "- **Overprocessing:** The original code applied several preprocessing steps (scaling, thresholding, blurring, etc.) before running OCR. While these steps can help with noisy or low-contrast images, they often **destroy clean, high-contrast text**—especially for Arabic, where fine details matter.\n",
    "- **Order of Operations:** The pipeline tried processed versions first, so if the original image was already optimal, it was never used for OCR.\n",
    "- **PSM/OEM Settings:** The code tried a limited set of Tesseract Page Segmentation Modes (PSM) and OCR Engine Modes (OEM), which may not have been optimal for all images.\n",
    "- **Text Cleaning:** The cleaning function was aggressive, but if Tesseract output was empty or too short, the result was discarded.\n",
    "\n",
    "## **What Was Changed to Achieve 100% Success**\n",
    "\n",
    "1. **Prioritize the Original Image:**  \n",
    "   The new code always tries the original, unprocessed grayscale image first, with several PSM settings. This ensures that clean images are not degraded by unnecessary processing.\n",
    "\n",
    "2. **Expanded Preprocessing (But Only If Needed):**  \n",
    "   Only if the original image fails, the code tries padded and scaled versions, but never applies destructive thresholding or blurring unless absolutely necessary.\n",
    "\n",
    "3. **Multiple PSM and OEM Combinations:**  \n",
    "   For each image variant, the code tries several PSM (6, 7, 3, 13) and both OEM (3, 1) settings, maximizing the chance that Tesseract will interpret the layout correctly.\n",
    "\n",
    "4. **Result Selection:**  \n",
    "   All non-empty results are collected, and the **longest valid extraction** is chosen, which is usually the correct full name.\n",
    "\n",
    "5. **Diagnostics:**  \n",
    "   Additional debug and diagnostic code was used to confirm that the original image, with minimal processing, consistently yields the best results for this dataset.\n",
    "\n",
    "# Reference\n",
    "\n",
    "The old (70%) code is left in the notebook for comparison. The new approach, as described above, achieves **100% extraction success** on the current dataset by respecting the quality of the input images and leveraging Tesseract's flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e532a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extractname(img_path):\n",
    "    \n",
    "#     # --- HELPER: TEXT CLEANER ---\n",
    "#     def clean_text(raw_text):\n",
    "#         if not raw_text: return \"\"\n",
    "#         # Keep Arabic letters (0621-064A) and spaces\n",
    "#         cleaned = re.sub(r'[^\\u0621-\\u064A\\s]', '', raw_text)\n",
    "#         cleaned = cleaned.replace('\\n', ' ')\n",
    "#         cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "#         return cleaned\n",
    "\n",
    "#     # --- LOAD IMAGE AS GRAYSCALE DIRECTLY ---\n",
    "#     img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     if img_gray is None: \n",
    "#         return \"\"\n",
    "\n",
    "#     # Try multiple approaches and collect all results\n",
    "#     all_results = []\n",
    "    \n",
    "#     # Preprocessing variants\n",
    "#     preprocessed_images = {\n",
    "#         'original': img_gray,\n",
    "#         'padded': cv2.copyMakeBorder(img_gray, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255),\n",
    "#     }\n",
    "    \n",
    "#     # Add scaled version\n",
    "#     h, w = img_gray.shape\n",
    "#     scaled = cv2.resize(img_gray, (w*2, h*2), interpolation=cv2.INTER_CUBIC)\n",
    "#     preprocessed_images['scaled_padded'] = cv2.copyMakeBorder(scaled, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255)\n",
    "    \n",
    "#     # PSM modes to try\n",
    "#     psm_modes = [6, 7, 3, 13]  # 13 = raw line\n",
    "    \n",
    "#     for img_name, img in preprocessed_images.items():\n",
    "#         for psm in psm_modes:\n",
    "#             for oem in [3, 1]:  # Try both LSTM+Legacy and LSTM only\n",
    "#                 try:\n",
    "#                     config = f\"--oem {oem} --psm {psm}\"\n",
    "#                     text = pytesseract.image_to_string(img, lang='ara', config=config)\n",
    "#                     cleaned = clean_text(text)\n",
    "                    \n",
    "#                     if len(cleaned) > 2:\n",
    "#                         all_results.append((cleaned, len(cleaned), img_name, psm, oem))\n",
    "#                 except:\n",
    "#                     continue\n",
    "    \n",
    "#     # Return the longest valid result\n",
    "#     if all_results:\n",
    "#         all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "#         return all_results[0][0]\n",
    "    \n",
    "#     return \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################    easyOCR       #####################################\n",
    "\n",
    "\n",
    "# --- EasyOCR (minimal) ---\n",
    "# Keeps ONLY the logic to reduce د <-> ل confusion via lexicon correction.\n",
    "# Preprocessing should be done in the pipeline before calling extractname().\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import easyocr\n",
    "\n",
    "USE_NAME_LEXICON_CORRECTION = True\n",
    "NAME_LEXICON_CSV = \"name_labels.csv\"\n",
    "\n",
    "EASYOCR_READTEXT_KWARGS = dict(\n",
    "    detail=1,\n",
    "    paragraph=False,\n",
    "    decoder=\"beamsearch\",\n",
    "    beamWidth=5,\n",
    "    batch_size=1,\n",
    "    text_threshold=0.55,\n",
    "    low_text=0.30,\n",
    "    link_threshold=0.35,\n",
    "    contrast_ths=0.08,\n",
    "    adjust_contrast=0.7,\n",
    "    mag_ratio=2.0,\n",
    " )\n",
    "\n",
    "_EASYOCR_READER = None\n",
    "_NAME_LEXICON = None\n",
    "\n",
    "\n",
    "def _ensure_easyocr_reader():\n",
    "    global _EASYOCR_READER\n",
    "    if _EASYOCR_READER is None:\n",
    "        _EASYOCR_READER = easyocr.Reader([\"ar\", \"en\"], gpu=True)\n",
    "    return _EASYOCR_READER\n",
    "\n",
    "\n",
    "def _load_name_lexicon():\n",
    "    global _NAME_LEXICON\n",
    "    if _NAME_LEXICON is not None:\n",
    "        return _NAME_LEXICON\n",
    "    lex = []\n",
    "    if os.path.exists(NAME_LEXICON_CSV):\n",
    "        try:\n",
    "            with open(NAME_LEXICON_CSV, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                for row in csv.DictReader(f):\n",
    "                    v = (row.get(\"transcription\") or row.get(\"Name\") or \"\").strip()\n",
    "                    if v:\n",
    "                        lex.append(v)\n",
    "        except Exception:\n",
    "            lex = []\n",
    "    _NAME_LEXICON = lex\n",
    "    return _NAME_LEXICON\n",
    "\n",
    "\n",
    "def _norm_dl(s: str) -> str:\n",
    "    # Treat د and ل as the same character for matching only\n",
    "    return (s or \"\").replace(\"د\", \"X\").replace(\"ل\", \"X\")\n",
    "\n",
    "\n",
    "def _levenshtein(a: str, b: str) -> int:\n",
    "    a = a or \"\"\n",
    "    b = b or \"\"\n",
    "    n, m = len(a), len(b)\n",
    "    if n == 0:\n",
    "        return m\n",
    "    if m == 0:\n",
    "        return n\n",
    "    prev = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        curr = [i] + [0] * m\n",
    "        ca = a[i - 1]\n",
    "        for j in range(1, m + 1):\n",
    "            cb = b[j - 1]\n",
    "            cost = 0 if ca == cb else 1\n",
    "            curr[j] = min(\n",
    "                prev[j] + 1,\n",
    "                curr[j - 1] + 1,\n",
    "                prev[j - 1] + cost,\n",
    "            )\n",
    "        prev = curr\n",
    "    return prev[m]\n",
    "\n",
    "\n",
    "def _correct_with_lexicon_dl(pred: str) -> str:\n",
    "    lex = _load_name_lexicon()\n",
    "    if not pred or not lex:\n",
    "        return pred\n",
    "    p = _norm_dl(pred)\n",
    "    best_name, best_dist = pred, 10**9\n",
    "    for cand in lex:\n",
    "        d = _levenshtein(p, _norm_dl(cand))\n",
    "        if d < best_dist:\n",
    "            best_dist, best_name = d, cand\n",
    "    # conservative accept threshold to avoid over-correcting\n",
    "    tol = max(2, int(0.18 * max(len(best_name), 1)))\n",
    "    return best_name if best_dist <= tol else pred\n",
    "\n",
    "\n",
    "def extractname(image_or_path, debug=False):\n",
    "    \"\"\"EasyOCR name extraction. Pass a preprocessed image from the pipeline for best results.\"\"\"\n",
    "    try:\n",
    "        reader = _ensure_easyocr_reader()\n",
    "        results = reader.readtext(image_or_path, **EASYOCR_READTEXT_KWARGS)\n",
    "\n",
    "        items = []\n",
    "        for r in results:\n",
    "            try:\n",
    "                bbox, text, conf = r\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "            text = text.strip()\n",
    "            if len(text) < 2:\n",
    "                continue\n",
    "            conf = float(conf) if conf is not None else 0.0\n",
    "            try:\n",
    "                cx = float(np.mean([p[0] for p in bbox]))\n",
    "            except Exception:\n",
    "                cx = 0.0\n",
    "            items.append((cx, text, conf))\n",
    "\n",
    "        # RTL-ish: sort by X descending (single-line name crops)\n",
    "        items.sort(key=lambda t: t[0], reverse=True)\n",
    "        joined = \" \".join([t for _, t, __ in items]).strip()\n",
    "\n",
    "        out = _correct_with_lexicon_dl(joined) if USE_NAME_LEXICON_CORRECTION else joined\n",
    "        if debug:\n",
    "            confs = [c for _, __, c in items]\n",
    "            avg_conf = float(np.mean(confs)) if confs else 0.0\n",
    "            print(f\"[EasyOCR] avg_conf={avg_conf:.3f} raw='{joined}' corrected='{out}'\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR Error on {type(image_or_path).__name__}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "#############################           PaddleOCR            ############################\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Avoid slow \"model hoster connectivity\" checks on startup\n",
    "# os.environ.setdefault(\"DISABLE_MODEL_SOURCE_CHECK\", \"True\")\n",
    "\n",
    "# from paddleocr import PaddleOCR\n",
    "\n",
    "# # PaddleOCR v3.x: `use_gpu` and `use_angle_cls` are not valid args.\n",
    "# # For orientation/angle handling, use `use_textline_orientation`.\n",
    "# # Bump text-det resolution and relax thresholds so small words aren't missed.\n",
    "# ocr = PaddleOCR(\n",
    "#     lang='ar',\n",
    "#     use_textline_orientation=True,\n",
    "#     text_det_limit_side_len=1280,\n",
    "#     text_det_limit_type='max',\n",
    "#     text_det_thresh=0.2,\n",
    "#     text_det_box_thresh=0.3,\n",
    "#     text_det_unclip_ratio=1.8,\n",
    "# )\n",
    "\n",
    "\n",
    "# def _poly_center(poly):\n",
    "#     # poly is expected to be (4,2) array-like\n",
    "#     try:\n",
    "#         xs = [float(p[0]) for p in poly]\n",
    "#         ys = [float(p[1]) for p in poly]\n",
    "#         return (sum(xs) / len(xs), sum(ys) / len(ys))\n",
    "#     except Exception:\n",
    "#         return (0.0, 0.0)\n",
    "\n",
    "\n",
    "# def _group_into_lines(items, y_tol=18.0):\n",
    "#     \"\"\"Group (cx, cy, text) into lines by y coordinate.\"\"\"\n",
    "#     items = sorted(items, key=lambda t: t[1])\n",
    "#     lines = []\n",
    "#     current = []\n",
    "#     current_y = None\n",
    "\n",
    "#     for cx, cy, text in items:\n",
    "#         if current_y is None:\n",
    "#             current_y = cy\n",
    "#             current = [(cx, cy, text)]\n",
    "#             continue\n",
    "\n",
    "#         if abs(cy - current_y) <= y_tol:\n",
    "#             current.append((cx, cy, text))\n",
    "#             current_y = (current_y * (len(current) - 1) + cy) / len(current)\n",
    "#         else:\n",
    "#             lines.append(current)\n",
    "#             current = [(cx, cy, text)]\n",
    "#             current_y = cy\n",
    "\n",
    "#     if current:\n",
    "#         lines.append(current)\n",
    "\n",
    "#     return lines\n",
    "\n",
    "\n",
    "# def extractname(image_or_path):\n",
    "#     \"\"\"Reads Arabic name text from a path or ndarray using PaddleOCR.\"\"\"\n",
    "#     try:\n",
    "#         src = image_or_path\n",
    "#         if isinstance(src, str):\n",
    "#             img = cv2.imread(src)\n",
    "#         else:\n",
    "#             img = src\n",
    "#         if img is None:\n",
    "#             return \"\"\n",
    "\n",
    "#         # Pad the crop a bit to avoid clipping the first/last word at the borders.\n",
    "#         img = cv2.copyMakeBorder(img, 10, 10, 70, 70, cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "\n",
    "#         result = ocr.ocr(img)\n",
    "\n",
    "#         tokens = []\n",
    "\n",
    "#         # PaddleOCR v3.x output shape: list[dict], with keys like `rec_texts`, `rec_polys`.\n",
    "#         if isinstance(result, list) and result and isinstance(result[0], dict):\n",
    "#             for page in result:\n",
    "#                 rec_texts = page.get('rec_texts') or []\n",
    "#                 rec_polys = page.get('rec_polys') or []\n",
    "\n",
    "#                 if isinstance(rec_texts, list) and isinstance(rec_polys, list) and len(rec_texts) == len(rec_polys):\n",
    "#                     items = []\n",
    "#                     for t, poly in zip(rec_texts, rec_polys):\n",
    "#                         if not isinstance(t, str):\n",
    "#                             continue\n",
    "#                         t = t.strip()\n",
    "#                         if not t:\n",
    "#                             continue\n",
    "#                         cx, cy = _poly_center(poly)\n",
    "#                         items.append((cx, cy, t))\n",
    "\n",
    "#                     # Group into lines by Y, then sort each line RTL (X desc)\n",
    "#                     for line in _group_into_lines(items):\n",
    "#                         line_sorted = sorted(line, key=lambda t: t[0], reverse=True)\n",
    "#                         tokens.extend([t for _, __, t in line_sorted])\n",
    "#                 else:\n",
    "#                     # Fallback: just take the text list order as-is\n",
    "#                     if isinstance(rec_texts, list):\n",
    "#                         tokens.extend([t.strip() for t in rec_texts if isinstance(t, str) and t.strip()])\n",
    "#                     elif isinstance(rec_texts, str) and rec_texts.strip():\n",
    "#                         tokens.append(rec_texts.strip())\n",
    "\n",
    "#         else:\n",
    "#             # Older output shape: [[ [box, (text, score)], ... ]]\n",
    "#             try:\n",
    "#                 for line in result:\n",
    "#                     for res in line:\n",
    "#                         t = res[1][0]\n",
    "#                         if isinstance(t, str) and t.strip():\n",
    "#                             tokens.append(t.strip())\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "\n",
    "#         # Clean: keep Arabic letters and spaces\n",
    "#         if tokens:\n",
    "#             joined = \" \".join(tokens)\n",
    "#             joined = re.sub(r'[^\\u0621-\\u064A\\s]', '', joined)\n",
    "#             joined = re.sub(r'\\s+', ' ', joined).strip()\n",
    "#             return joined\n",
    "\n",
    "#         return \"\"\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"PaddleOCR Error on {type(image_or_path).__name__}: {e}\")\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1acdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pipeline_accuracy(true_file_path, extracted_file_path):\n",
    "    def _norm_arabic_variants(s: str) -> str:\n",
    "        # Treat: ي == ى, and ه == ة (normalize to ي and ه)\n",
    "        return (s or \"\").replace(\"ى\", \"ي\").replace(\"ة\", \"ه\")\n",
    "\n",
    "    try:\n",
    "        df_true = pd.read_excel(true_file_path)\n",
    "        df_extracted = pd.read_excel(extracted_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    df_true.columns = df_true.columns.str.strip()\n",
    "    df_extracted.columns = df_extracted.columns.str.strip()\n",
    "\n",
    "    min_len = min(len(df_true), len(df_extracted))\n",
    "    df_true = df_true.iloc[:min_len].reset_index(drop=True)\n",
    "    df_extracted = df_extracted.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "    columns_to_check = ['Code', 'Daf3', 'Name']\n",
    "    scores = {}\n",
    "\n",
    "    print(f\"--- Accuracy Report (Checking {min_len} rows) ---\\n\")\n",
    "\n",
    "    for col in columns_to_check:\n",
    "        if col not in df_true.columns or col not in df_extracted.columns:\n",
    "            print(f\"Error: Column '{col}' missing.\")\n",
    "            print(f\"   Available in True File: {df_true.columns.tolist()}\")\n",
    "            print(f\"   Available in Extracted: {df_extracted.columns.tolist()}\")\n",
    "            scores[col] = 0.0\n",
    "            continue\n",
    "\n",
    "        true_series = df_true[col].astype(str).fillna('')\n",
    "        extracted_series = df_extracted[col].astype(str).fillna('')\n",
    "\n",
    "        true_clean = true_series.str.strip().str.replace(r'\\.0$', '', regex=True)\n",
    "        extracted_clean = extracted_series.str.strip().str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "        if col == 'Name':\n",
    "            row_scores = []\n",
    "\n",
    "            for t_val, e_val in zip(true_clean, extracted_clean):\n",
    "                t_val = _norm_arabic_variants(t_val)\n",
    "                e_val = _norm_arabic_variants(e_val)\n",
    "\n",
    "                t_nospace = t_val.replace(\" \", \"\")\n",
    "                e_nospace = e_val.replace(\" \", \"\")\n",
    "\n",
    "                if t_val == e_val:\n",
    "                    row_scores.append(1.0)\n",
    "                elif (t_nospace == e_nospace) and (abs(len(t_val) - len(e_val)) <= 1):\n",
    "                    row_scores.append(1.0)\n",
    "                else:\n",
    "                    t_words = set(t_val.split())\n",
    "                    e_words = set(e_val.split())\n",
    "                    if len(t_words) == 0:\n",
    "                        row_scores.append(1.0 if len(e_words) == 0 else 0.0)\n",
    "                    else:\n",
    "                        common = t_words.intersection(e_words)\n",
    "                        row_scores.append(len(common) / len(t_words))\n",
    "\n",
    "            accuracy = np.mean(row_scores) * 100\n",
    "        else:\n",
    "            matches = (true_clean == extracted_clean)\n",
    "            accuracy = (matches.sum() / len(matches)) * 100\n",
    "\n",
    "        scores[col] = accuracy\n",
    "        print(f\"{col} Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    average_accuracy = (sum(scores.values()) / len(scores)) if scores else 0.0\n",
    "\n",
    "    print(f\"\\n--------------------------------\")\n",
    "    print(f\"AVERAGE ACCURACY: {average_accuracy:.2f}%\")\n",
    "    print(f\"--------------------------------\")\n",
    "\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117bf62",
   "metadata": {},
   "source": [
    "# **Main Pipeline** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40718274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 149 training images...\n",
      "Training Complete. Validation Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython.display import display, Image as IPyImage, HTML\n",
    "\n",
    "def show_image_cv(img, title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    if len(img.shape) == 2:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main_pipeline():\n",
    "    base_dir = os.getcwd()\n",
    "    path_to_dataset = os.path.join(base_dir, 'Raw_IDs')\n",
    "    refrence_image_path = os.path.join(base_dir, 'Raw_IDs', 'ID14.jpg')\n",
    "\n",
    "    SVMclassifier = train_SVM_robust()\n",
    "\n",
    "    data_for_excel = []\n",
    "\n",
    "    # Safety check for directory\n",
    "    if not os.path.exists(path_to_dataset):\n",
    "        print(f\"Directory not found: {path_to_dataset}\")\n",
    "        return\n",
    "\n",
    "    for i in os.listdir(path_to_dataset):\n",
    "        if not i.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(path_to_dataset, i)\n",
    "\n",
    "        # --- Processing ---\n",
    "        raw_img = cv2.imread(img_path)\n",
    "        clean_img, is_impulsive = is_impulsive_noise(raw_img)\n",
    "        aligned_img = align_images_sift(clean_img, refrence_image_path)\n",
    "        clean_img2, is_random = is_random_noise(aligned_img)\n",
    "        enhanced_img = enhance_contrast_clahe(clean_img2)\n",
    "        # Simple sharpening kernel\n",
    "        kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 5,-1],\n",
    "                   [0, -1, 0]])\n",
    "        sharpened = cv2.filter2D(enhanced_img, -1, kernel)\n",
    "        name_img, digit_imgs, daf3_digits = extract_name_and_digits(sharpened)\n",
    "        student_id = os.path.splitext(i)[0]\n",
    "\n",
    "        # Save images\n",
    "        save_student_name(student_id, name_img)\n",
    "        save_split_digits(student_id, digit_imgs)\n",
    "        save_split_digits(f\"{student_id}_daf3\", daf3_digits, output_folder=\"extracted_daf3_digits\")\n",
    "\n",
    "        # Predict Code\n",
    "        digit_preds = []\n",
    "        for digit_img in digit_imgs:\n",
    "            feat = extract_hog_features(digit_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            digit_preds.append(str(pred))\n",
    "        code_str = ''.join(digit_preds)\n",
    "\n",
    "        # Predict Daf3\n",
    "        daf3_preds = []\n",
    "        for d_img in daf3_digits:\n",
    "            feat = extract_hog_features(d_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            daf3_preds.append(str(pred))\n",
    "        daf3_str = ''.join(daf3_preds)\n",
    "\n",
    "        # Extract Name\n",
    "        name_text = extractname(f'./extracted_names/{student_id}_name.jpg')\n",
    "\n",
    "        data_for_excel.append({\n",
    "            \"Student ID\": student_id,   \n",
    "            \"Name\": name_text,\n",
    "            \"Code\": code_str,\n",
    "            \"Daf3\": daf3_str,\n",
    "        })\n",
    "\n",
    "    # --- OUTPUT SECTION ---\n",
    "    if data_for_excel:\n",
    "        df = pd.DataFrame(data_for_excel)\n",
    "        df = df.sort_values(by=\"Student ID\", key=lambda x: x.str.extract(r'(\\d+)').iloc[:, 0].astype(int))\n",
    "\n",
    "        print(\"Processing Complete. Results:\")\n",
    "        true_results_path = 'True_Results.xlsx'\n",
    "\n",
    "        df_display = df[['Student ID', 'Name', 'Code', 'Daf3']].copy()\n",
    "        if os.path.exists(true_results_path):\n",
    "            try:\n",
    "                true_df = pd.read_excel(true_results_path)\n",
    "                true_df.columns = true_df.columns.str.strip()\n",
    "\n",
    "                def _key(c: str) -> str:\n",
    "                    return re.sub(r\"[^a-z0-9]\", \"\", str(c).strip().lower())\n",
    "\n",
    "                colmap = {_key(c): c for c in true_df.columns}\n",
    "                sid_col = colmap.get('studentid') or colmap.get('student_id') or colmap.get('id')\n",
    "                if sid_col is None:\n",
    "                    raise KeyError(f\"No student-id column found. Columns: {list(true_df.columns)}\")\n",
    "                true_df = true_df.rename(columns={sid_col: 'Student ID'})\n",
    "\n",
    "                name_col = colmap.get('name')\n",
    "                code_col = colmap.get('code')\n",
    "                daf3_col = colmap.get('daf3')\n",
    "                if name_col and name_col != 'Name':\n",
    "                    true_df = true_df.rename(columns={name_col: 'Name'})\n",
    "                if code_col and code_col != 'Code':\n",
    "                    true_df = true_df.rename(columns={code_col: 'Code'})\n",
    "                if daf3_col and daf3_col != 'Daf3':\n",
    "                    true_df = true_df.rename(columns={daf3_col: 'Daf3'})\n",
    "\n",
    "                left = df_display.copy()\n",
    "                right = true_df.copy()\n",
    "                left['_sid_key'] = left['Student ID'].astype(str).str.extract(r'(\\d+)')[0]\n",
    "                right['_sid_key'] = right['Student ID'].astype(str).str.extract(r'(\\d+)')[0]\n",
    "                merged = left.merge(right, on='_sid_key', how='left', suffixes=('', '_True'))\n",
    "\n",
    "                def _norm_series(s: pd.Series) -> pd.Series:\n",
    "                    return s.astype(str).fillna('').str.strip().str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "                merged['Name_Correct'] = _norm_series(merged['Name']) == _norm_series(merged.get('Name_True', ''))\n",
    "                merged['Code_Correct'] = _norm_series(merged['Code']) == _norm_series(merged.get('Code_True', ''))\n",
    "                merged['Daf3_Correct'] = _norm_series(merged['Daf3']) == _norm_series(merged.get('Daf3_True', ''))\n",
    "\n",
    "                df_display = merged[[\n",
    "                    'Student ID',\n",
    "                    'Name', 'Name_Correct',\n",
    "                    'Code', 'Code_Correct',\n",
    "                    'Daf3', 'Daf3_Correct',\n",
    "                ]]\n",
    "            except Exception as e:\n",
    "                print(f\"Could not add correctness columns: {e}\")\n",
    "\n",
    "        display(df_display)\n",
    "\n",
    "        #SAVE TO EXCEL\n",
    "        output_file = \"Extracted_Results.xlsx\"\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "\n",
    "            worksheet = writer.sheets['Sheet1']\n",
    "            for column in df:\n",
    "                column_length = max(df[column].astype(str).map(len).max(), len(column))\n",
    "                col_idx = df.columns.get_loc(column)\n",
    "                col_letter = chr(65 + col_idx)\n",
    "                worksheet.column_dimensions[col_letter].width = column_length + 2\n",
    "\n",
    "\n",
    "        print(f\"Excel file saved to: {output_file}\")\n",
    "        \n",
    "        true_results_path = 'True_Results.xlsx' \n",
    "        \n",
    "        if os.path.exists(true_results_path):\n",
    "            calculate_pipeline_accuracy(true_results_path, output_file)\n",
    "        else:\n",
    "            print(f\"Skipping accuracy check: '{true_results_path}' not found.\")\n",
    "\n",
    "main_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886914b",
   "metadata": {},
   "source": [
    "## Qwen2.5-VL pretrained OCR (comparison only)\n",
    "- This section does **not** change your pipeline or existing OCR code.\n",
    "- It runs a pretrained Qwen2.5-VL OCR model on the already-saved name crops in `extracted_names/` and writes a separate Excel file for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ff21031",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCUDA is not available. Fix torch CUDA then restart the kernel.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m model = \u001b[43mQwen2_5_VLForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     75\u001b[39m     processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m, use_fast=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\WNDOWS\\anaconda3\\envs\\imageproc_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\WNDOWS\\anaconda3\\envs\\imageproc_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:5029\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5027\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5029\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\WNDOWS\\anaconda3\\envs\\imageproc_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:1365\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1362\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\WNDOWS\\anaconda3\\envs\\imageproc_gpu\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:127\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "\n",
    "try:\n",
    "    import torch.compiler as _torch_compiler\n",
    "    if not hasattr(_torch_compiler, \"is_compiling\"):\n",
    "        if hasattr(torch, \"_dynamo\") and hasattr(torch._dynamo, \"is_compiling\"):\n",
    "            _torch_compiler.is_compiling = torch._dynamo.is_compiling\n",
    "        else:\n",
    "            _torch_compiler.is_compiling = lambda: False\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "MODEL_NAME = \"sherif1313/Arabic-handwritten-OCR-4bit-Qwen2.5-VL-3B-v2\"\n",
    "NAMES_DIR = \"extracted_names\"\n",
    "BASELINE_XLSX = \"Extracted_Results.xlsx\"\n",
    "TRUE_XLSX = \"True_Results.xlsx\"\n",
    "QWEN_XLSX = \"Qwen_Results.xlsx\"\n",
    "\n",
    "\n",
    "def _clean_arabic_name(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[^\\u0621-\\u064A\\s]\", \" \", str(text))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _pipeline_name_score(true_name: str, extracted_name: str) -> float:\n",
    "    t = \"\" if true_name is None else str(true_name)\n",
    "    e = \"\" if extracted_name is None else str(extracted_name)\n",
    "    t = t.strip()\n",
    "    e = e.strip()\n",
    "    if t.endswith(\".0\"):\n",
    "        t = t[:-2]\n",
    "    if e.endswith(\".0\"):\n",
    "        e = e[:-2]\n",
    "    if t == e:\n",
    "        return 1.0\n",
    "    t_nospace = t.replace(\" \", \"\")\n",
    "    e_nospace = e.replace(\" \", \"\")\n",
    "    if (t_nospace == e_nospace) and (abs(len(t) - len(e)) <= 1):\n",
    "        return 1.0\n",
    "    t_words = set(t.split())\n",
    "    e_words = set(e.split())\n",
    "    if len(t_words) == 0:\n",
    "        return 1.0 if len(e_words) == 0 else 0.0\n",
    "    return len(t_words.intersection(e_words)) / len(t_words)\n",
    "\n",
    "\n",
    "def _pipeline_name_accuracy(true_names, extracted_names) -> float:\n",
    "    scores = [_pipeline_name_score(t, e) for t, e in zip(true_names, extracted_names)]\n",
    "    return float(np.mean(scores) * 100) if scores else 0.0\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Fix torch CUDA then restart the kernel.\")\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    " )\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=False)\n",
    "except TypeError:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def extract_name_from_image(image_path: str) -> str:\n",
    "    prompt = \"استخرج اسم الطالب فقط من الصورة. اكتب الاسم العربي فقط بدون أرقام أو رموز أو كلمات إضافية.\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        pad_token_id=processor.tokenizer.eos_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    )\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids[:, input_len:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )[0]\n",
    "    return _clean_arabic_name(output_text)\n",
    "\n",
    "\n",
    "if not os.path.isdir(NAMES_DIR):\n",
    "    raise RuntimeError(f\"{NAMES_DIR!r} folder not found. Run the main pipeline first.\")\n",
    "if not os.path.exists(BASELINE_XLSX):\n",
    "    raise RuntimeError(f\"{BASELINE_XLSX!r} not found. Run the main pipeline first.\")\n",
    "\n",
    "df_base = pd.read_excel(BASELINE_XLSX)\n",
    "df_base.columns = df_base.columns.str.strip()\n",
    "if \"Student ID\" not in df_base.columns:\n",
    "    raise RuntimeError(f\"{BASELINE_XLSX!r} is missing 'Student ID' column.\")\n",
    "\n",
    "qwen_rows = []\n",
    "for sid in df_base[\"Student ID\"].astype(str).tolist():\n",
    "    p1 = os.path.join(NAMES_DIR, f\"{sid}_name.jpg\")\n",
    "    p2 = os.path.join(NAMES_DIR, f\"{sid}.jpg\")\n",
    "    img_path = p1 if os.path.exists(p1) else p2\n",
    "    qwen_name = extract_name_from_image(img_path) if os.path.exists(img_path) else \"\"\n",
    "    qwen_rows.append({\"Student ID\": sid, \"Name\": qwen_name})\n",
    "\n",
    "df_qwen = pd.DataFrame(qwen_rows)\n",
    "df_qwen.to_excel(QWEN_XLSX, index=False)\n",
    "\n",
    "if os.path.exists(TRUE_XLSX):\n",
    "    df_true = pd.read_excel(TRUE_XLSX)\n",
    "    df_true.columns = df_true.columns.str.strip()\n",
    "    min_len = min(len(df_true), len(df_base), len(df_qwen))\n",
    "    actual = df_true.get(\"Name\", pd.Series([\"\"] * min_len)).iloc[:min_len].tolist()\n",
    "    mycode = df_base.get(\"Name\", pd.Series([\"\"] * min_len)).iloc[:min_len].tolist()\n",
    "    qwen = df_qwen.get(\"Name\", pd.Series([\"\"] * min_len)).iloc[:min_len].tolist()\n",
    "\n",
    "    baseline_acc = _pipeline_name_accuracy(actual, mycode)\n",
    "    qwen_acc = _pipeline_name_accuracy(actual, qwen)\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        \"Actual Name\": actual,\n",
    "        \"My Code Output\": mycode,\n",
    "        \"Qwen Output\": qwen,\n",
    "    })\n",
    "    print(f\"Baseline Name Accuracy: {baseline_acc:.2f}%\")\n",
    "    print(f\"Qwen Name Accuracy:     {qwen_acc:.2f}%\")\n",
    "    print(f\"Name Delta (baseline - Qwen): {baseline_acc - qwen_acc:+.2f} points\")\n",
    "    display(table.head(30))\n",
    "else:\n",
    "    display(df_qwen.head(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageproc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
