{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba08ac",
   "metadata": {},
   "source": [
    "## **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10cf11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pytesseract\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "from openpyxl.utils import get_column_letter  # Add this line\n",
    "from commonfunctions import *\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' # (Windows Example)\n",
    "\n",
    "\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab16d",
   "metadata": {},
   "source": [
    "## **Main Functions Overview**\n",
    "\n",
    "- **Image Alignment**\n",
    "  - Detects SIFT keypoints and descriptors in the input and reference images.\n",
    "  - Matches them using the ratio test.\n",
    "  - Uses RANSAC to estimate a homography.\n",
    "  - Applies the homography to warp the input image so it lines up with the reference.\n",
    "  - Returns the aligned image (or the original if not enough matches are found).\n",
    "\n",
    "- **Extract Details**\n",
    "  - Uses (x, y, w, h) coordinates to crop the aligned card into:\n",
    "    - The name region\n",
    "    - The code (ID) region\n",
    "  - Returns these sub-images for downstream OCR or digit processing.\n",
    "\n",
    "- **Save Student Name**\n",
    "  - Ensures the output folder exists.\n",
    "  - Writes the cropped name image to disk with a filename that includes the student ID.\n",
    "  - Creates a persistent record usable for manual review or OCR.\n",
    "\n",
    "- **Split and Save Digits**\n",
    "  - Converts the code region to grayscale and applies Otsu thresholding.\n",
    "  - Finds contours and filters out small noise.\n",
    "  - Selects the largest seven contours (by area) and sorts them left-to-right.\n",
    "  - Saves each detected digit crop into a per-student folder as individual image files.\n",
    "\n",
    "- **save_split_digits**\n",
    "  - Takes a list of digit images for a student.\n",
    "  - Ensures a folder exists for each student (named by their ID).\n",
    "  - Saves each digit image as `digit_0.jpg`, `digit_1.jpg`, ..., `digit_6.jpg` inside the student’s folder.\n",
    "  - Used for batch saving when all digit crops are already extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945bafe",
   "metadata": {},
   "source": [
    "## **Noise Detection and Treatment**\n",
    "- **Impulsive Noise (Median Filter)**\n",
    "- **Random Noise (Gaussian Filter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f54ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_random_noise(img, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Detects if an image has random (Gaussian) noise.\n",
    "    If noise is above threshold, applies Gaussian blurring.\n",
    "    Returns the (possibly filtered) image and a boolean indicating if noise was detected and treated image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Estimate noise using a simple method: standard deviation of pixel intensities\n",
    "    stddev = np.std(img)\n",
    "    normalized_stddev = stddev / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    if normalized_stddev < threshold:\n",
    "        return img, False  # No significant noise, return original\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred_img = cv2.GaussianBlur(img, (7, 7), 0)\n",
    "    #show_images([img,blurred_img], [\"Original Image\", \"After Gaussian Blur\"])\n",
    "    return blurred_img, True\n",
    "\n",
    "# gray_img = cv2.imread('ykismail_College-ID-Scanner_main_images/ID10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# noisy_img = random_noise(gray_img, mode='gaussian', mean=0.5)\n",
    "# noisy_img = (noisy_img * 255).astype('uint8')  # Convert back to uint8 for OpenCV\n",
    "\n",
    "# TreatedImg, noise_detected = is_random_noise(noisy_img)\n",
    "# if noise_detected:\n",
    "#     print(\"Random noise detected and treated!\")\n",
    "# else:\n",
    "#     print(\"No random noise detected.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53737cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_impulsive_noise(img, threshold=0.1, black_range=(0, 9), white_range=(246, 255)):\n",
    "    \"\"\"\n",
    "    Detects if an image has impulsive (salt-and-pepper) noise.\n",
    "    If noise is above threshold, applies median filtering with adaptive kernel size.\n",
    "    Returns the (possibly filtered) image and a boolean indicating if noise was detected an treted image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    total_pixels = img.size\n",
    "    num_black = np.sum((img >= black_range[0]) & (img <= black_range[1]))\n",
    "    num_white = np.sum((img >= white_range[0]) & (img <= white_range[1]))\n",
    "    prop = (num_black + num_white) / total_pixels\n",
    "\n",
    "    if prop < threshold:\n",
    "        return img, False  # No significant noise, return original\n",
    "    # Determine kernel size based on noise severity\n",
    "    k = int(3 + prop * 10)\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    k = min(max(k, 3), 9)\n",
    "    filtered_img = cv2.medianBlur(img, k)\n",
    "    #show_images([img,filtered_img], [\"Original Image\", \"After median filter\"])\n",
    "    return filtered_img, True\n",
    "\n",
    "# gray_img = cv2.imread('ykismail_College-ID-Scanner_main_images/ID10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# noisy_img = random_noise(gray_img, mode='s&p', amount=0.6)\n",
    "# noisy_img = (noisy_img * 255).astype('uint8')  # Convert back to uint8 for OpenCV\n",
    "\n",
    "# TreatedImg, noise_detected = is_impulsive_noise(noisy_img)\n",
    "# if noise_detected:\n",
    "#     print(\"Impulsive noise detected and treated!\")\n",
    "# else:\n",
    "#     print(\"No impulsive noise detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf03c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images_sift(img_to_align, reference_path):\n",
    "    img1 = img_to_align\n",
    "    img2 = cv2.imread(reference_path)      # Train Image (The perfect template)\n",
    "    \n",
    "    # --- FIX: Check if img1 is already grayscale ---\n",
    "    if len(img1.shape) == 3:\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray1 = img1 # Already grayscale\n",
    "\n",
    "    # Ref image is loaded from disk, usually BGR, but good to check\n",
    "    if len(img2.shape) == 3:\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray2 = img2\n",
    "\n",
    "    sift = cv2.SIFT_create() \n",
    "    \n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > 10:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        h, w = img2.shape[:2]\n",
    "        # Warp the original img1 (whether gray or color)\n",
    "        aligned_img = cv2.warpPerspective(img1, M, (w, h))\n",
    "\n",
    "        return aligned_img\n",
    "    \n",
    "    else:\n",
    "        print(f\"Not enough matches found: {len(good_matches)}/10\")\n",
    "        return img1\n",
    "    \n",
    "\n",
    "\n",
    "def extract_details(aligned_image):\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    \n",
    "    name_contour = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_contour = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    \n",
    "    return name_contour, code_contour\n",
    "\n",
    "\n",
    "def save_student_name(student_id, name_img, output_folder=\"extracted_names\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    # Construct filename: extracted_names/ID1_name.jpg\n",
    "    filename = f\"{output_folder}/{student_id}_name.jpg\"\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(filename, name_img)\n",
    "    \n",
    "\n",
    "def split_and_save_digits(student_id, code_roi, output_folder=\"extracted_digits\"):\n",
    "    save_path = f\"{output_folder}/ID{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    gray = cv2.cvtColor(code_roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # A. Collect all valid candidates\n",
    "    candidates = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        # Filter tiny noise\n",
    "        if h > 15 and w > 5:\n",
    "            candidates.append((x, y, w, h, area))\n",
    "            \n",
    "    # B. CRITICAL: Select exactly the top 7 by AREA (Size)\n",
    "    # This removes small specks or the colon \":\" if it was caught\n",
    "    candidates = sorted(candidates, key=lambda c: c[4], reverse=True) # Sort largest first\n",
    "    final_digits = candidates[:7] # Take top 7\n",
    "    \n",
    "    # C. Sort the final 7 by X-COORDINATE (Left -> Right)\n",
    "    # This puts them back in the correct reading order (1, 2, 3...)\n",
    "    final_digits = sorted(final_digits, key=lambda c: c[0])\n",
    "        \n",
    "    # D. Save\n",
    "    for index, (x, y, w, h, area) in enumerate(final_digits):\n",
    "        digit_img = code_roi[y:y+h, x:x+w]\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)\n",
    "\n",
    "      \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_name_and_digits(aligned_image):\n",
    "    \"\"\"\n",
    "    Input: An aligned ID card image.\n",
    "    Output: \n",
    "      - name_roi: The image of the extracted name.\n",
    "      - code_digits: A list of images for the code digits.\n",
    "      - daf3_digits: A list of images for the daf3 digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Hardcoded Coordinates ---\n",
    "    # x, y, w, h\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    daf3_coords = (350, 500, 620, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    dx, dy, dw, dh = daf3_coords\n",
    "    \n",
    "    # Extract ROIs\n",
    "    name_img = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_roi = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    daf3_img = aligned_image[dy:dy+dh, dx:dx+dw]\n",
    "    \n",
    "    # --- Helper Function to Process Any ROI ---\n",
    "    def process_roi_digits(roi_img, digit_limit):\n",
    "        \"\"\"\n",
    "        Applies grayscale, thresholding, contour detection, \n",
    "        splitting of merged digits, and sorting.\n",
    "        \"\"\"\n",
    "        #gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if len(roi_img.shape) == 3:\n",
    "            gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = roi_img\n",
    "        # --- FIX ENDS HERE --\n",
    "        \n",
    "        # Binary Inverse + Otsu\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        candidates = []\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            area = w * h\n",
    "            \n",
    "            # Filter tiny noise \n",
    "            if h > 15 and w > 5:\n",
    "                \n",
    "                # --- CHECK FOR MERGED DIGITS ---\n",
    "                # If width > 0.8 * height, likely two digits stuck together\n",
    "                if w > 0.8 * h: \n",
    "                    half_w = w // 2\n",
    "                    # Digit 1 (Left half)\n",
    "                    candidates.append((x, y, half_w, h, half_w * h))\n",
    "                    # Digit 2 (Right half)\n",
    "                    candidates.append((x + half_w, y, half_w, h, half_w * h))\n",
    "                else:\n",
    "                    # Normal single digit\n",
    "                    candidates.append((x, y, w, h, area))\n",
    "        \n",
    "        # 1. Sort by Area Descending (Keep only the largest objects to remove noise)\n",
    "        candidates = sorted(candidates, key=lambda c: c[4], reverse=True)[:digit_limit]\n",
    "        \n",
    "        # 2. Sort by X-coordinate Ascending (Order them Left -> Right)\n",
    "        final_candidates = sorted(candidates, key=lambda c: c[0])\n",
    "        \n",
    "        # Crop the actual images\n",
    "        cropped_digits = []\n",
    "        for (x, y, w, h, area) in final_candidates:\n",
    "            digit_crop = roi_img[y:y+h, x:x+w]\n",
    "            cropped_digits.append(digit_crop)\n",
    "            \n",
    "        return cropped_digits\n",
    "\n",
    "    # --- 2. Process Regions ---\n",
    "    \n",
    "    # Detect Code (Limit 7 digits)\n",
    "    code_digits = process_roi_digits(code_roi, digit_limit=7)\n",
    "    \n",
    "    # Detect Daf3 (Limit 14 digits)\n",
    "    daf3_digits = process_roi_digits(daf3_img, digit_limit=14)\n",
    "\n",
    "    return name_img, code_digits, daf3_digits\n",
    "\n",
    "def save_split_digits(student_id, digit_imgs, output_folder=\"extracted_digits\"):\n",
    "    \"\"\"\n",
    "    Saves a list of digit images for a student in the same way as split_and_save_digits.\n",
    "    Each digit is saved as digit_0.jpg, digit_1.jpg, ..., digit_6.jpg in a folder per student.\n",
    "    \"\"\"\n",
    "    save_path = f\"{output_folder}/{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for index, digit_img in enumerate(digit_imgs):\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f17014",
   "metadata": {},
   "source": [
    "## **SVM English Number Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff7086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_dataset = r\"train_digits\"  # Training set\n",
    "\n",
    "def train_SVM_robust():\n",
    "    # 1. Map your specific filename prefixes to actual digits\n",
    "    label_map = {\n",
    "        'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', \n",
    "        'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '9'\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    img_filenames = os.listdir(path_to_train_dataset)\n",
    "    print(f\"Loading {len(img_filenames)} training images...\")\n",
    "\n",
    "    for fn in img_filenames:\n",
    "        if not fn.lower().endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "\n",
    "        # Get the first letter (a, b, c...)\n",
    "        prefix = fn[0].lower()\n",
    "        if prefix in label_map:\n",
    "            labels.append(label_map[prefix])\n",
    "            \n",
    "            path = os.path.join(path_to_train_dataset, fn)\n",
    "            img = cv2.imread(path)\n",
    "            \n",
    "            # Extract HOG features (ensure preprocessing matches)\n",
    "            features.append(extract_hog_features(img))\n",
    "    \n",
    "    # 2. Create a Pipeline: Scale Features -> Train SVM\n",
    "    # Scaling is CRITICAL for HOG-based SVMs\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', LinearSVC(random_state=42, max_iter=5000, dual=False))\n",
    "    ])\n",
    "    \n",
    "    # 3. Train/Test Split for internal validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(f\"Training Complete. Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    # Ensure grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # APPLY IDENTICAL PREPROCESSING TO TRAINING AND TEST DATA\n",
    "    # This turns both sets into \"binary masks\" to ignore lighting/shadows\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32)) # target_img_size\n",
    "    \n",
    "    win_size = (32, 32)\n",
    "    cell_size = (8, 8)  # Slightly larger cells help ignore \"noise/shadows\"\n",
    "    block_size = (16, 16)\n",
    "    block_stride = (8, 8)\n",
    "    nbins = 9\n",
    "    \n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fbfe0",
   "metadata": {},
   "source": [
    "# **Tesseract Arabic OCR**\n",
    "\n",
    "## **Current Situation**\n",
    "\n",
    "The project uses Tesseract OCR to extract Arabic names from scanned images. Initially, the extraction pipeline achieved only a **70% success rate**. This meant that about 30% of the images failed to yield any valid Arabic text, even though the images were visually clear and contained readable names.\n",
    "\n",
    "## **Why Was the Success Rate Only 70%?**\n",
    "\n",
    "- **Overprocessing:** The original code applied several preprocessing steps (scaling, thresholding, blurring, etc.) before running OCR. While these steps can help with noisy or low-contrast images, they often **destroy clean, high-contrast text**—especially for Arabic, where fine details matter.\n",
    "- **Order of Operations:** The pipeline tried processed versions first, so if the original image was already optimal, it was never used for OCR.\n",
    "- **PSM/OEM Settings:** The code tried a limited set of Tesseract Page Segmentation Modes (PSM) and OCR Engine Modes (OEM), which may not have been optimal for all images.\n",
    "- **Text Cleaning:** The cleaning function was aggressive, but if Tesseract output was empty or too short, the result was discarded.\n",
    "\n",
    "## **What Was Changed to Achieve 100% Success**\n",
    "\n",
    "1. **Prioritize the Original Image:**  \n",
    "   The new code always tries the original, unprocessed grayscale image first, with several PSM settings. This ensures that clean images are not degraded by unnecessary processing.\n",
    "\n",
    "2. **Expanded Preprocessing (But Only If Needed):**  \n",
    "   Only if the original image fails, the code tries padded and scaled versions, but never applies destructive thresholding or blurring unless absolutely necessary.\n",
    "\n",
    "3. **Multiple PSM and OEM Combinations:**  \n",
    "   For each image variant, the code tries several PSM (6, 7, 3, 13) and both OEM (3, 1) settings, maximizing the chance that Tesseract will interpret the layout correctly.\n",
    "\n",
    "4. **Result Selection:**  \n",
    "   All non-empty results are collected, and the **longest valid extraction** is chosen, which is usually the correct full name.\n",
    "\n",
    "5. **Diagnostics:**  \n",
    "   Additional debug and diagnostic code was used to confirm that the original image, with minimal processing, consistently yields the best results for this dataset.\n",
    "\n",
    "# Reference\n",
    "\n",
    "The old (70%) code is left in the notebook for comparison. The new approach, as described above, achieves **100% extraction success** on the current dataset by respecting the quality of the input images and leveraging Tesseract's flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e532a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the folder 'test_arabic_names_full' was not found\n"
     ]
    }
   ],
   "source": [
    "def extractname(img_path):\n",
    "    \n",
    "    # --- HELPER: TEXT CLEANER ---\n",
    "    def clean_text(raw_text):\n",
    "        if not raw_text: return \"\"\n",
    "        # Keep Arabic letters (0621-064A) and spaces\n",
    "        cleaned = re.sub(r'[^\\u0621-\\u064A\\s]', '', raw_text)\n",
    "        cleaned = cleaned.replace('\\n', ' ')\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        return cleaned\n",
    "\n",
    "    # --- LOAD IMAGE AS GRAYSCALE DIRECTLY ---\n",
    "    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_gray is None: \n",
    "        return \"\"\n",
    "\n",
    "    # Try multiple approaches and collect all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Preprocessing variants\n",
    "    preprocessed_images = {\n",
    "        'original': img_gray,\n",
    "        'padded': cv2.copyMakeBorder(img_gray, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255),\n",
    "    }\n",
    "    \n",
    "    # Add scaled version\n",
    "    h, w = img_gray.shape\n",
    "    scaled = cv2.resize(img_gray, (w*2, h*2), interpolation=cv2.INTER_CUBIC)\n",
    "    preprocessed_images['scaled_padded'] = cv2.copyMakeBorder(scaled, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255)\n",
    "    \n",
    "    # PSM modes to try\n",
    "    psm_modes = [6, 7, 3, 13]  # 13 = raw line\n",
    "    \n",
    "    for img_name, img in preprocessed_images.items():\n",
    "        for psm in psm_modes:\n",
    "            for oem in [3, 1]:  # Try both LSTM+Legacy and LSTM only\n",
    "                try:\n",
    "                    config = f\"--oem {oem} --psm {psm}\"\n",
    "                    text = pytesseract.image_to_string(img, lang='ara', config=config)\n",
    "                    cleaned = clean_text(text)\n",
    "                    \n",
    "                    if len(cleaned) > 2:\n",
    "                        all_results.append((cleaned, len(cleaned), img_name, psm, oem))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Return the longest valid result\n",
    "    if all_results:\n",
    "        all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return all_results[0][0]\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "folder_path = 'test_arabic_names_full'\n",
    "data = []\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"Processing images in: {folder_path}...\\n\")\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is an image\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            extracted_text = extractname(full_path)    \n",
    "            clean_text_result = extracted_text.strip()\n",
    "            data.append({'Filename': filename, 'Extracted Name': clean_text_result})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ===== SUCCESS RATE CALCULATION =====\n",
    "    total_images = len(df)\n",
    "    successful_extractions = len(df[df['Extracted Name'] != ''])\n",
    "    failed_extractions = total_images - successful_extractions\n",
    "    success_rate = (successful_extractions / total_images) * 100 if total_images > 0 else 0\n",
    "    \n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"OCR EXTRACTION RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total Images Processed: {total_images}\")\n",
    "    print(f\"Successful Extractions: {successful_extractions}\")\n",
    "    print(f\"Failed Extractions:     {failed_extractions}\")\n",
    "    print(f\"Success Rate:           {success_rate:.2f}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Show failed images\n",
    "    if failed_extractions > 0:\n",
    "        failed_df = df[df['Extracted Name'] == '']\n",
    "        print(\"Failed to extract text from:\")\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['Filename']}\")\n",
    "        print()\n",
    "    \n",
    "    display(df.head(50))\n",
    "    \n",
    "else:\n",
    "    print(f\"the folder '{folder_path}' was not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117bf62",
   "metadata": {},
   "source": [
    "# **Main Pipeline** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40718274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 149 training images...\n",
      "Training Complete. Validation Accuracy: 100.00%\n",
      "Processing Complete. Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Code</th>\n",
       "      <th>Daf3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مصطفى محمود احمد عويس</td>\n",
       "      <td>1200277</td>\n",
       "      <td>14712020100041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>يوسف عمرو سيد كمال محمود محمد</td>\n",
       "      <td>1220319</td>\n",
       "      <td>14712022101322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يوسف محمد حمزه محمد عبدالتواب</td>\n",
       "      <td>1220092</td>\n",
       "      <td>14712022100089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ظ عمر محمد محمود صفوت احمد سعد الدين</td>\n",
       "      <td>1220188</td>\n",
       "      <td>14712022100080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ظ عبد الرحمن على السيد محمد السيد الحديدى</td>\n",
       "      <td>1220062</td>\n",
       "      <td>14712022100036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>محمد تامر عبدالعزيز سالم</td>\n",
       "      <td>4230189</td>\n",
       "      <td>14712022101553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>فادى سامى نبيل داود</td>\n",
       "      <td>4230160</td>\n",
       "      <td>14712022100643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ظ احمد مدحت عبدالعزيز على عوض ح ته</td>\n",
       "      <td>4230142</td>\n",
       "      <td>14712022101079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>مروان عمرو عيدالسجبيه زاك احسده لكر ى</td>\n",
       "      <td>177770</td>\n",
       "      <td>147187101875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>عبدالله محمد جمال الدين احمد مصطفى</td>\n",
       "      <td>1220256</td>\n",
       "      <td>14712022101354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>رش وليه مصد تيه فرج سن</td>\n",
       "      <td>1220123</td>\n",
       "      <td>14712022101537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ظ شاميه علاء محمد سيد احمد</td>\n",
       "      <td>122017</td>\n",
       "      <td>1471202210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>لدين حمد عفيفي كريم محمد نجم الدين حمدي</td>\n",
       "      <td>1220137</td>\n",
       "      <td>14712022101624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ظ عمرو ايهاب مختار فرحات</td>\n",
       "      <td>4230159</td>\n",
       "      <td>14712022101610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ع سارة محمد مصطفى جوده زهير</td>\n",
       "      <td>1220125</td>\n",
       "      <td>14712022101262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>يوسف جلال محمد نور الدين جلال</td>\n",
       "      <td>1200309</td>\n",
       "      <td>14712020100046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>أ محمد تامر عبدالعزيز سالم</td>\n",
       "      <td>4230189</td>\n",
       "      <td>14712022101553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>عست حم طيق مون</td>\n",
       "      <td>1200883</td>\n",
       "      <td>15054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>سم احمد ابراه السروى</td>\n",
       "      <td>1190533</td>\n",
       "      <td>14712019101776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>اية احمد محمد خشبه</td>\n",
       "      <td>1230165</td>\n",
       "      <td>14712023101223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>احمد هشام سيد سالم</td>\n",
       "      <td>1220003</td>\n",
       "      <td>14712022100623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ظ حنين محمد اسماعيل محمد حماده</td>\n",
       "      <td>1220121</td>\n",
       "      <td>14712022101552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>اسمهان ابراهيم يوسف البيطار</td>\n",
       "      <td>1220165</td>\n",
       "      <td>14712022101214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ظ امير سليم سليمان سليمان موسى</td>\n",
       "      <td>1210096</td>\n",
       "      <td>14712021100205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>محمد عبدالرحيم سالم عبدالهادي</td>\n",
       "      <td>1230315</td>\n",
       "      <td>14712023101217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ظ حسن حسام حسن حامد عبد الفتاح</td>\n",
       "      <td>1210216</td>\n",
       "      <td>14712021101550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>عنيءة اد خ عن لما دكدججيبه ي عاك</td>\n",
       "      <td>1210227</td>\n",
       "      <td>14712021101455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>اية احمد محمد خشبه</td>\n",
       "      <td>1230165</td>\n",
       "      <td>14712023101223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>سلمى احمد عبدا بدالرحيم محمد</td>\n",
       "      <td>1210235</td>\n",
       "      <td>14712021101553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ظ ابراهيم محمود ابراهيم عبدالمعطى</td>\n",
       "      <td>1180568</td>\n",
       "      <td>14712018102167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>محمود نصر محمود يحي كلب</td>\n",
       "      <td>1200454</td>\n",
       "      <td>14712020101893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>سلمى محمد ابراهيم فتحى ابوريده ظ</td>\n",
       "      <td>1230196</td>\n",
       "      <td>14712023101286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ا مروان عمرو عبدالمجيد فؤاد احمد شكرى</td>\n",
       "      <td>1220279</td>\n",
       "      <td>14712022101373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>علي ع ا ارييسة</td>\n",
       "      <td>1220145</td>\n",
       "      <td>14712022101495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>حسن عمرو مصطفي محمد يوسسف</td>\n",
       "      <td>1230030</td>\n",
       "      <td>14712023101060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>انمد سامح صبحي لغتاةطتية سا</td>\n",
       "      <td>1274460</td>\n",
       "      <td>14712020102030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>فهيم عبدالفتاح محمد احمد فهيم عد</td>\n",
       "      <td>1220273</td>\n",
       "      <td>14712022101338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>أحمد محمد سيد صابر محمد</td>\n",
       "      <td>1230144</td>\n",
       "      <td>14712023100148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>احمد محمد حسين الكومى هر</td>\n",
       "      <td>1220221</td>\n",
       "      <td>14712022101628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ايددبي معرس يوج</td>\n",
       "      <td>1210284</td>\n",
       "      <td>14712021101365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ظ جمانة عمرو مصطفى عبد الصالح عرابي</td>\n",
       "      <td>1220229</td>\n",
       "      <td>14712022101345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ظ حنين محمد اسماعيل محمد حماده</td>\n",
       "      <td>1220121</td>\n",
       "      <td>14712022101552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ادهم احمد محمود ممدوح شبانة</td>\n",
       "      <td>1230157</td>\n",
       "      <td>14712023101281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>جنى ايمن وفائى محمد عيسم</td>\n",
       "      <td>5210349</td>\n",
       "      <td>14724021101568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>حسين محمد ماهر بهاءالدين</td>\n",
       "      <td>1220237</td>\n",
       "      <td>14712022101358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>مصيلحى احمد سامح صلاح</td>\n",
       "      <td>4230138</td>\n",
       "      <td>14712022101505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>كمال محمد منير عبدالحميد</td>\n",
       "      <td>4230174</td>\n",
       "      <td>14712022101524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>يوسف محمد حمدى محمد عثمان</td>\n",
       "      <td>1220301</td>\n",
       "      <td>14712222101392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ظ معاد امام احمد امام احمد</td>\n",
       "      <td>1220205</td>\n",
       "      <td>14712022101329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ا كا</td>\n",
       "      <td>1220149</td>\n",
       "      <td>14712022101620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name     Code            Daf3\n",
       "0                       مصطفى محمود احمد عويس  1200277  14712020100041\n",
       "1               يوسف عمرو سيد كمال محمود محمد  1220319  14712022101322\n",
       "2               يوسف محمد حمزه محمد عبدالتواب  1220092  14712022100089\n",
       "3        ظ عمر محمد محمود صفوت احمد سعد الدين  1220188  14712022100080\n",
       "4   ظ عبد الرحمن على السيد محمد السيد الحديدى  1220062  14712022100036\n",
       "5                    محمد تامر عبدالعزيز سالم  4230189  14712022101553\n",
       "6                         فادى سامى نبيل داود  4230160  14712022100643\n",
       "7          ظ احمد مدحت عبدالعزيز على عوض ح ته  4230142  14712022101079\n",
       "8       مروان عمرو عيدالسجبيه زاك احسده لكر ى   177770    147187101875\n",
       "9          عبدالله محمد جمال الدين احمد مصطفى  1220256  14712022101354\n",
       "10                     رش وليه مصد تيه فرج سن  1220123  14712022101537\n",
       "11                 ظ شاميه علاء محمد سيد احمد   122017      1471202210\n",
       "12    لدين حمد عفيفي كريم محمد نجم الدين حمدي  1220137  14712022101624\n",
       "13                   ظ عمرو ايهاب مختار فرحات  4230159  14712022101610\n",
       "14                ع سارة محمد مصطفى جوده زهير  1220125  14712022101262\n",
       "15              يوسف جلال محمد نور الدين جلال  1200309  14712020100046\n",
       "16                 أ محمد تامر عبدالعزيز سالم  4230189  14712022101553\n",
       "17                             عست حم طيق مون  1200883           15054\n",
       "18                       سم احمد ابراه السروى  1190533  14712019101776\n",
       "19                         اية احمد محمد خشبه  1230165  14712023101223\n",
       "20                         احمد هشام سيد سالم  1220003  14712022100623\n",
       "21             ظ حنين محمد اسماعيل محمد حماده  1220121  14712022101552\n",
       "22                اسمهان ابراهيم يوسف البيطار  1220165  14712022101214\n",
       "23             ظ امير سليم سليمان سليمان موسى  1210096  14712021100205\n",
       "24              محمد عبدالرحيم سالم عبدالهادي  1230315  14712023101217\n",
       "25             ظ حسن حسام حسن حامد عبد الفتاح  1210216  14712021101550\n",
       "26           عنيءة اد خ عن لما دكدججيبه ي عاك  1210227  14712021101455\n",
       "27                         اية احمد محمد خشبه  1230165  14712023101223\n",
       "28               سلمى احمد عبدا بدالرحيم محمد  1210235  14712021101553\n",
       "29          ظ ابراهيم محمود ابراهيم عبدالمعطى  1180568  14712018102167\n",
       "30                    محمود نصر محمود يحي كلب  1200454  14712020101893\n",
       "31           سلمى محمد ابراهيم فتحى ابوريده ظ  1230196  14712023101286\n",
       "32      ا مروان عمرو عبدالمجيد فؤاد احمد شكرى  1220279  14712022101373\n",
       "33                             علي ع ا ارييسة  1220145  14712022101495\n",
       "34                  حسن عمرو مصطفي محمد يوسسف  1230030  14712023101060\n",
       "35                انمد سامح صبحي لغتاةطتية سا  1274460  14712020102030\n",
       "36           فهيم عبدالفتاح محمد احمد فهيم عد  1220273  14712022101338\n",
       "37                    أحمد محمد سيد صابر محمد  1230144  14712023100148\n",
       "38                   احمد محمد حسين الكومى هر  1220221  14712022101628\n",
       "39                            ايددبي معرس يوج  1210284  14712021101365\n",
       "40        ظ جمانة عمرو مصطفى عبد الصالح عرابي  1220229  14712022101345\n",
       "41             ظ حنين محمد اسماعيل محمد حماده  1220121  14712022101552\n",
       "42                ادهم احمد محمود ممدوح شبانة  1230157  14712023101281\n",
       "43                   جنى ايمن وفائى محمد عيسم  5210349  14724021101568\n",
       "44                   حسين محمد ماهر بهاءالدين  1220237  14712022101358\n",
       "45                      مصيلحى احمد سامح صلاح  4230138  14712022101505\n",
       "46                   كمال محمد منير عبدالحميد  4230174  14712022101524\n",
       "47                  يوسف محمد حمدى محمد عثمان  1220301  14712222101392\n",
       "48                 ظ معاد امام احمد امام احمد  1220205  14712022101329\n",
       "49                                       ا كا  1220149  14712022101620"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved to: Extracted_Results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# specific import for Jupyter Notebooks\n",
    "from IPython.display import display \n",
    "\n",
    "def main_pipeline():\n",
    "    base_dir = os.getcwd()\n",
    "    path_to_dataset = os.path.join(base_dir, 'Raw_IDs')\n",
    "    refrence_image_path = os.path.join(base_dir, 'Raw_IDs', 'ID14.jpg')\n",
    "    \n",
    "    # Ensure the classifier is trained\n",
    "    SVMclassifier = train_SVM_robust()\n",
    "    \n",
    "    data_for_excel = [] \n",
    "\n",
    "    # Safety check for directory\n",
    "    if not os.path.exists(path_to_dataset):\n",
    "        print(f\"Directory not found: {path_to_dataset}\")\n",
    "        return\n",
    "\n",
    "    for i in os.listdir(path_to_dataset):\n",
    "        if not i.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(path_to_dataset, i)\n",
    "        \n",
    "        # --- Processing ---\n",
    "        raw_img = cv2.imread(img_path)\n",
    "        clean_img, is_impulsive = is_impulsive_noise(raw_img)\n",
    "        #clean_img, is_random = is_random_noise(clean_img)\n",
    "        aligned_img = align_images_sift(clean_img, refrence_image_path)\n",
    "        name_img, digit_imgs, daf3_digits = extract_name_and_digits(aligned_img)\n",
    "        \n",
    "        student_id = os.path.splitext(i)[0]\n",
    "        \n",
    "        # Save images\n",
    "        save_student_name(student_id, name_img)\n",
    "        save_split_digits(student_id, digit_imgs)\n",
    "        save_split_digits(f\"{student_id}_daf3\", daf3_digits, output_folder=\"extracted_daf3_digits\")\n",
    "        \n",
    "        # Predict Code\n",
    "        digit_preds = []\n",
    "        for digit_img in digit_imgs:\n",
    "            feat = extract_hog_features(digit_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            digit_preds.append(str(pred))\n",
    "        code_str = ''.join(digit_preds)\n",
    "        \n",
    "        # Predict Daf3\n",
    "        daf3_preds = []\n",
    "        for d_img in daf3_digits:\n",
    "            feat = extract_hog_features(d_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            daf3_preds.append(str(pred))\n",
    "        daf3_str = ''.join(daf3_preds)\n",
    "\n",
    "        # Extract Name\n",
    "        name_text = extractname(f'./extracted_names/{student_id}_name.jpg')\n",
    "        \n",
    "        # Add to list\n",
    "        data_for_excel.append({\n",
    "            \"Student ID\": student_id,\n",
    "            \"Name\": name_text,\n",
    "            \"Code\": code_str,\n",
    "            \"Daf3\": daf3_str,\n",
    "        })\n",
    "\n",
    "    # --- OUTPUT SECTION ---\n",
    "    if data_for_excel:\n",
    "        df = pd.DataFrame(data_for_excel)\n",
    "        \n",
    "        # 1. DISPLAY TABLE IN JUPYTER\n",
    "        print(\"Processing Complete. Results:\")\n",
    "        # This renders the DataFrame as a nice HTML table in the output cell\n",
    "        # We filter to show only Name, Code, and Daf3 as requested\n",
    "        display(df[['Name', 'Code', 'Daf3']])\n",
    "        \n",
    "        # 2. SAVE TO EXCEL\n",
    "        output_file = \"Extracted_Results.xlsx\"\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "            \n",
    "            # Auto-resize columns\n",
    "            worksheet = writer.sheets['Sheet1']\n",
    "            for column in df:\n",
    "                column_length = max(df[column].astype(str).map(len).max(), len(column))\n",
    "                col_idx = df.columns.get_loc(column)\n",
    "                col_letter = chr(65 + col_idx)\n",
    "                worksheet.column_dimensions[col_letter].width = column_length + 2\n",
    "                \n",
    "        print(f\"Excel file saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No data processed.\")\n",
    "\n",
    "# Run it\n",
    "main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
