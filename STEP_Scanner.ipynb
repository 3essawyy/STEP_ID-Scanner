{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba08ac",
   "metadata": {},
   "source": [
    "## **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10cf11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import imutils  # If you are unable to install this library, ask the TA; we only need this in extract_hsv_histogram.\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pytesseract\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import random_noise\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "from openpyxl.utils import get_column_letter  # Add this line\n",
    "from commonfunctions import *\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe' # (Windows Example)\n",
    "\n",
    "\n",
    "target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "# We are going to fix the random seed to make our experiments reproducible \n",
    "# since some algorithms use pseudorandom generators\n",
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ab16d",
   "metadata": {},
   "source": [
    "## **Main Functions Overview**\n",
    "\n",
    "- **Image Alignment**\n",
    "  - Detects SIFT keypoints and descriptors in the input and reference images.\n",
    "  - Matches them using the ratio test.\n",
    "  - Uses RANSAC to estimate a homography.\n",
    "  - Applies the homography to warp the input image so it lines up with the reference.\n",
    "  - Returns the aligned image (or the original if not enough matches are found).\n",
    "\n",
    "- **Extract Details**\n",
    "  - Uses (x, y, w, h) coordinates to crop the aligned card into:\n",
    "    - The name region\n",
    "    - The code (ID) region\n",
    "  - Returns these sub-images for downstream OCR or digit processing.\n",
    "\n",
    "- **Save Student Name**\n",
    "  - Ensures the output folder exists.\n",
    "  - Writes the cropped name image to disk with a filename that includes the student ID.\n",
    "  - Creates a persistent record usable for manual review or OCR.\n",
    "\n",
    "- **Split and Save Digits**\n",
    "  - Converts the code region to grayscale and applies Otsu thresholding.\n",
    "  - Finds contours and filters out small noise.\n",
    "  - Selects the largest seven contours (by area) and sorts them left-to-right.\n",
    "  - Saves each detected digit crop into a per-student folder as individual image files.\n",
    "\n",
    "- **save_split_digits**\n",
    "  - Takes a list of digit images for a student.\n",
    "  - Ensures a folder exists for each student (named by their ID).\n",
    "  - Saves each digit image as `digit_0.jpg`, `digit_1.jpg`, ..., `digit_6.jpg` inside the student’s folder.\n",
    "  - Used for batch saving when all digit crops are already extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf03c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images_sift(image_path, reference_path):\n",
    "    img1 = cv2.imread(image_path)          # Query Image (The messy card)\n",
    "    img2 = cv2.imread(reference_path)      # Train Image (The perfect template)\n",
    "    \n",
    "    # Convert to grayscale for SIFT\n",
    "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    \n",
    "    sift = cv2.SIFT_create() \n",
    "    \n",
    "        \n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "    # SIFT descriptors are continuous, so we use the default Norm (L2), not Hamming\n",
    "    bf = cv2.BFMatcher()\n",
    "\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    \n",
    "    # Loop through the matches (m is best match, n is second best)\n",
    "    for m, n in matches:\n",
    "        # If the distance of the best match is less than 0.75 of the second best...\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    # This helps you see if the lines are parallel (good) or crossing (bad)\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=2)\n",
    "    \n",
    "    if len(good_matches) > 10:\n",
    "        # Extract location of good matches\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Find the Homography Matrix\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        # Apply the Warp\n",
    "        h, w = img2.shape[:2]\n",
    "        aligned_img = cv2.warpPerspective(img1, M, (w, h))\n",
    "\n",
    "        return aligned_img\n",
    "    \n",
    "    else:\n",
    "        print(f\"Not enough matches found: {len(good_matches)}/10\")\n",
    "        return img1\n",
    "    \n",
    "\n",
    "\n",
    "def extract_details(aligned_image):\n",
    "    name_coords = (100, 205, 1200, 150)\n",
    "    code_coords = (640, 404, 335, 110)\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    \n",
    "    name_contour = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_contour = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    \n",
    "    return name_contour, code_contour\n",
    "\n",
    "\n",
    "def save_student_name(student_id, name_img, output_folder=\"extracted_names\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    # Construct filename: extracted_names/ID1_name.jpg\n",
    "    filename = f\"{output_folder}/{student_id}_name.jpg\"\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(filename, name_img)\n",
    "    \n",
    "\n",
    "def split_and_save_digits(student_id, code_roi, output_folder=\"extracted_digits\"):\n",
    "    save_path = f\"{output_folder}/ID{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    gray = cv2.cvtColor(code_roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # A. Collect all valid candidates\n",
    "    candidates = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        # Filter tiny noise\n",
    "        if h > 15 and w > 5:\n",
    "            candidates.append((x, y, w, h, area))\n",
    "            \n",
    "    # B. CRITICAL: Select exactly the top 7 by AREA (Size)\n",
    "    # This removes small specks or the colon \":\" if it was caught\n",
    "    candidates = sorted(candidates, key=lambda c: c[4], reverse=True) # Sort largest first\n",
    "    final_digits = candidates[:7] # Take top 7\n",
    "    \n",
    "    # C. Sort the final 7 by X-COORDINATE (Left -> Right)\n",
    "    # This puts them back in the correct reading order (1, 2, 3...)\n",
    "    final_digits = sorted(final_digits, key=lambda c: c[0])\n",
    "        \n",
    "    # D. Save\n",
    "    for index, (x, y, w, h, area) in enumerate(final_digits):\n",
    "        digit_img = code_roi[y:y+h, x:x+w]\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)\n",
    "\n",
    "      \n",
    "\n",
    "def extract_name_and_digits(aligned_image):\n",
    "    \"\"\"\n",
    "    Input: An aligned ID card image.\n",
    "    Output: \n",
    "      - name_roi: The image of the extracted name.\n",
    "      - digit_imgs: A list of 7 images (one for each digit).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Hardcoded Coordinates ---\n",
    "    name_coords = (100, 205, 1200, 150)  # x, y, w, h\n",
    "    code_coords = (640, 404, 335, 110)   # x, y, w, h\n",
    "    \n",
    "    nx, ny, nw, nh = name_coords\n",
    "    cx, cy, cw, ch = code_coords\n",
    "    \n",
    "    # Extract the main ROIs\n",
    "    name_img = aligned_image[ny:ny+nh, nx:nx+nw]\n",
    "    code_roi = aligned_image[cy:cy+ch, cx:cx+cw]\n",
    "    \n",
    "    # --- 2. Process Code Block to Find Digits ---\n",
    "    gray = cv2.cvtColor(code_roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Optional: Small erosion to separate touching digits\n",
    "    # kernel = np.ones((2,2), np.uint8)\n",
    "    # gray = cv2.erode(gray, kernel, iterations=1)\n",
    "    \n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter candidates by size & SPLIT merged digits\n",
    "    candidates = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        \n",
    "        # Filter tiny noise\n",
    "        if h > 15 and w > 5:\n",
    "            \n",
    "            # --- THE FIX: CHECK FOR MERGED DIGITS ---\n",
    "            # If the width is roughly equal to or greater than height, \n",
    "            # it's likely two digits stuck together.\n",
    "            # (Standard digits are usually tall rectangles, w < 0.6*h)\n",
    "            if w > 0.8 * h: \n",
    "                # Split this box into two halves\n",
    "                half_w = w // 2\n",
    "                \n",
    "                # Digit 1 (Left half)\n",
    "                candidates.append((x, y, half_w, h, half_w * h))\n",
    "                \n",
    "                # Digit 2 (Right half)\n",
    "                candidates.append((x + half_w, y, half_w, h, half_w * h))\n",
    "                \n",
    "            else:\n",
    "                # It's a normal single digit\n",
    "                candidates.append((x, y, w, h, area))\n",
    "            \n",
    "    # CRITICAL: Sort by Area (Top 7) then by X-coordinate (Left->Right)\n",
    "    candidates = sorted(candidates, key=lambda c: c[4], reverse=True)[:7] # Top 7 largest\n",
    "    final_digits = sorted(candidates, key=lambda c: c[0])                 # Sort Left to Right\n",
    "    \n",
    "    # Crop the actual digit images\n",
    "    digit_imgs = []\n",
    "    for (x, y, w, h, area) in final_digits:\n",
    "        digit_crop = code_roi[y:y+h, x:x+w]\n",
    "        digit_imgs.append(digit_crop)\n",
    "        \n",
    "    return name_img, digit_imgs\n",
    "\n",
    "def save_split_digits(student_id, digit_imgs, output_folder=\"extracted_digits\"):\n",
    "    \"\"\"\n",
    "    Saves a list of digit images for a student in the same way as split_and_save_digits.\n",
    "    Each digit is saved as digit_0.jpg, digit_1.jpg, ..., digit_6.jpg in a folder per student.\n",
    "    \"\"\"\n",
    "    save_path = f\"{output_folder}/{student_id}\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for index, digit_img in enumerate(digit_imgs):\n",
    "        filename = f\"{save_path}/digit_{index}.jpg\"\n",
    "        cv2.imwrite(filename, digit_img)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4cae9",
   "metadata": {},
   "source": [
    "## **Noise Detection and Treatment**\n",
    "- **Impulsive Noise (Median Filter)**\n",
    "- **Random Noise (Gaussian Filter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53737cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_impulsive_noise(img, threshold=0.1, black_range=(0, 9), white_range=(246, 255)):\n",
    "    \"\"\"\n",
    "    Detects if an image has impulsive (salt-and-pepper) noise.\n",
    "    If noise is above threshold, applies median filtering with adaptive kernel size.\n",
    "    Returns the (possibly filtered) image and a boolean indicating if noise was detected an treted image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    total_pixels = img.size\n",
    "    num_black = np.sum((img >= black_range[0]) & (img <= black_range[1]))\n",
    "    num_white = np.sum((img >= white_range[0]) & (img <= white_range[1]))\n",
    "    prop = (num_black + num_white) / total_pixels\n",
    "\n",
    "    if prop < threshold:\n",
    "        return img, False  # No significant noise, return original\n",
    "    # Determine kernel size based on noise severity\n",
    "    k = int(3 + prop * 10)\n",
    "    if k % 2 == 0:\n",
    "        k += 1\n",
    "    k = min(max(k, 3), 9)\n",
    "    filtered_img = cv2.medianBlur(img, k)\n",
    "    return filtered_img, True\n",
    "\n",
    "# gray_img = cv2.imread('ykismail_College-ID-Scanner_main_images/ID10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# noisy_img = random_noise(gray_img, mode='s&p', amount=0.6)\n",
    "# noisy_img = (noisy_img * 255).astype('uint8')  # Convert back to uint8 for OpenCV\n",
    "\n",
    "# TreatedImg, noise_detected = is_impulsive_noise(noisy_img)\n",
    "# if noise_detected:\n",
    "#     print(\"Impulsive noise detected and treated!\")\n",
    "# else:\n",
    "#     print(\"No impulsive noise detected.\")\n",
    "\n",
    "# show_images([gray_img, noisy_img, TreatedImg], [\"Original Image\", \"Noisy Image\", \"After Median Filter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f54ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_random_noise(img, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Detects if an image has random (Gaussian) noise.\n",
    "    If noise is above threshold, applies Gaussian blurring.\n",
    "    Returns the (possibly filtered) image and a boolean indicating if noise was detected and treated image.\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Estimate noise using a simple method: standard deviation of pixel intensities\n",
    "    stddev = np.std(img)\n",
    "    normalized_stddev = stddev / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    if normalized_stddev < threshold:\n",
    "        return img, False  # No significant noise, return original\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred_img = cv2.GaussianBlur(img, (7, 7), 0)\n",
    "    return blurred_img, True\n",
    "\n",
    "# gray_img = cv2.imread('ykismail_College-ID-Scanner_main_images/ID10.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# noisy_img = random_noise(gray_img, mode='gaussian', mean=0.5)\n",
    "# noisy_img = (noisy_img * 255).astype('uint8')  # Convert back to uint8 for OpenCV\n",
    "\n",
    "# TreatedImg, noise_detected = is_random_noise(noisy_img)\n",
    "# if noise_detected:\n",
    "#     print(\"Random noise detected and treated!\")\n",
    "# else:\n",
    "#     print(\"No random noise detected.\")\n",
    "\n",
    "# show_images([gray_img, noisy_img, TreatedImg], [\"Original Image\", \"Noisy Image\", \"After Gaussian Blur\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f17014",
   "metadata": {},
   "source": [
    "## **SVM English Number Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ff7086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_dataset = r\"train_digits\"  # Training set\n",
    "\n",
    "def train_SVM_robust():\n",
    "    # 1. Map your specific filename prefixes to actual digits\n",
    "    label_map = {\n",
    "        'a': '0', 'b': '1', 'c': '2', 'd': '3', 'e': '4', \n",
    "        'f': '5', 'g': '6', 'h': '7', 'i': '8', 'j': '9'\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    img_filenames = os.listdir(path_to_train_dataset)\n",
    "    print(f\"Loading {len(img_filenames)} training images...\")\n",
    "\n",
    "    for fn in img_filenames:\n",
    "        if not fn.lower().endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "\n",
    "        # Get the first letter (a, b, c...)\n",
    "        prefix = fn[0].lower()\n",
    "        if prefix in label_map:\n",
    "            labels.append(label_map[prefix])\n",
    "            \n",
    "            path = os.path.join(path_to_train_dataset, fn)\n",
    "            img = cv2.imread(path)\n",
    "            \n",
    "            # Extract HOG features (ensure preprocessing matches)\n",
    "            features.append(extract_hog_features(img))\n",
    "    \n",
    "    # 2. Create a Pipeline: Scale Features -> Train SVM\n",
    "    # Scaling is CRITICAL for HOG-based SVMs\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', LinearSVC(random_state=42, max_iter=5000, dual=False))\n",
    "    ])\n",
    "    \n",
    "    # 3. Train/Test Split for internal validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(f\"Training Complete. Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    # Ensure grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # APPLY IDENTICAL PREPROCESSING TO TRAINING AND TEST DATA\n",
    "    # This turns both sets into \"binary masks\" to ignore lighting/shadows\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32)) # target_img_size\n",
    "    \n",
    "    win_size = (32, 32)\n",
    "    cell_size = (8, 8)  # Slightly larger cells help ignore \"noise/shadows\"\n",
    "    block_size = (16, 16)\n",
    "    block_stride = (8, 8)\n",
    "    nbins = 9\n",
    "    \n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fbfe0",
   "metadata": {},
   "source": [
    "# **Tesseract Arabic OCR**\n",
    "\n",
    "## **Current Situation**\n",
    "\n",
    "The project uses Tesseract OCR to extract Arabic names from scanned images. Initially, the extraction pipeline achieved only a **70% success rate**. This meant that about 30% of the images failed to yield any valid Arabic text, even though the images were visually clear and contained readable names.\n",
    "\n",
    "## **Why Was the Success Rate Only 70%?**\n",
    "\n",
    "- **Overprocessing:** The original code applied several preprocessing steps (scaling, thresholding, blurring, etc.) before running OCR. While these steps can help with noisy or low-contrast images, they often **destroy clean, high-contrast text**—especially for Arabic, where fine details matter.\n",
    "- **Order of Operations:** The pipeline tried processed versions first, so if the original image was already optimal, it was never used for OCR.\n",
    "- **PSM/OEM Settings:** The code tried a limited set of Tesseract Page Segmentation Modes (PSM) and OCR Engine Modes (OEM), which may not have been optimal for all images.\n",
    "- **Text Cleaning:** The cleaning function was aggressive, but if Tesseract output was empty or too short, the result was discarded.\n",
    "\n",
    "## **What Was Changed to Achieve 100% Success**\n",
    "\n",
    "1. **Prioritize the Original Image:**  \n",
    "   The new code always tries the original, unprocessed grayscale image first, with several PSM settings. This ensures that clean images are not degraded by unnecessary processing.\n",
    "\n",
    "2. **Expanded Preprocessing (But Only If Needed):**  \n",
    "   Only if the original image fails, the code tries padded and scaled versions, but never applies destructive thresholding or blurring unless absolutely necessary.\n",
    "\n",
    "3. **Multiple PSM and OEM Combinations:**  \n",
    "   For each image variant, the code tries several PSM (6, 7, 3, 13) and both OEM (3, 1) settings, maximizing the chance that Tesseract will interpret the layout correctly.\n",
    "\n",
    "4. **Result Selection:**  \n",
    "   All non-empty results are collected, and the **longest valid extraction** is chosen, which is usually the correct full name.\n",
    "\n",
    "5. **Diagnostics:**  \n",
    "   Additional debug and diagnostic code was used to confirm that the original image, with minimal processing, consistently yields the best results for this dataset.\n",
    "\n",
    "# Reference\n",
    "\n",
    "The old (70%) code is left in the notebook for comparison. The new approach, as described above, achieves **100% extraction success** on the current dataset by respecting the quality of the input images and leveraging Tesseract's flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e532a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the folder 'test_arabic_names_full' was not found\n"
     ]
    }
   ],
   "source": [
    "def extractname(img_path):\n",
    "    \n",
    "    # --- HELPER: TEXT CLEANER ---\n",
    "    def clean_text(raw_text):\n",
    "        if not raw_text: return \"\"\n",
    "        # Keep Arabic letters (0621-064A) and spaces\n",
    "        cleaned = re.sub(r'[^\\u0621-\\u064A\\s]', '', raw_text)\n",
    "        cleaned = cleaned.replace('\\n', ' ')\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        return cleaned\n",
    "\n",
    "    # --- LOAD IMAGE AS GRAYSCALE DIRECTLY ---\n",
    "    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_gray is None: \n",
    "        return \"\"\n",
    "\n",
    "    # Try multiple approaches and collect all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Preprocessing variants\n",
    "    preprocessed_images = {\n",
    "        'original': img_gray,\n",
    "        'padded': cv2.copyMakeBorder(img_gray, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255),\n",
    "    }\n",
    "    \n",
    "    # Add scaled version\n",
    "    h, w = img_gray.shape\n",
    "    scaled = cv2.resize(img_gray, (w*2, h*2), interpolation=cv2.INTER_CUBIC)\n",
    "    preprocessed_images['scaled_padded'] = cv2.copyMakeBorder(scaled, 40, 40, 40, 40, cv2.BORDER_CONSTANT, value=255)\n",
    "    \n",
    "    # PSM modes to try\n",
    "    psm_modes = [6, 7, 3, 13]  # 13 = raw line\n",
    "    \n",
    "    for img_name, img in preprocessed_images.items():\n",
    "        for psm in psm_modes:\n",
    "            for oem in [3, 1]:  # Try both LSTM+Legacy and LSTM only\n",
    "                try:\n",
    "                    config = f\"--oem {oem} --psm {psm}\"\n",
    "                    text = pytesseract.image_to_string(img, lang='ara', config=config)\n",
    "                    cleaned = clean_text(text)\n",
    "                    \n",
    "                    if len(cleaned) > 2:\n",
    "                        all_results.append((cleaned, len(cleaned), img_name, psm, oem))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Return the longest valid result\n",
    "    if all_results:\n",
    "        all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return all_results[0][0]\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "folder_path = 'test_arabic_names_full'\n",
    "data = []\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"Processing images in: {folder_path}...\\n\")\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is an image\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            extracted_text = extractname(full_path)    \n",
    "            clean_text_result = extracted_text.strip()\n",
    "            data.append({'Filename': filename, 'Extracted Name': clean_text_result})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ===== SUCCESS RATE CALCULATION =====\n",
    "    total_images = len(df)\n",
    "    successful_extractions = len(df[df['Extracted Name'] != ''])\n",
    "    failed_extractions = total_images - successful_extractions\n",
    "    success_rate = (successful_extractions / total_images) * 100 if total_images > 0 else 0\n",
    "    \n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"OCR EXTRACTION RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total Images Processed: {total_images}\")\n",
    "    print(f\"Successful Extractions: {successful_extractions}\")\n",
    "    print(f\"Failed Extractions:     {failed_extractions}\")\n",
    "    print(f\"Success Rate:           {success_rate:.2f}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Show failed images\n",
    "    if failed_extractions > 0:\n",
    "        failed_df = df[df['Extracted Name'] == '']\n",
    "        print(\"Failed to extract text from:\")\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['Filename']}\")\n",
    "        print()\n",
    "    \n",
    "    display(df.head(50))\n",
    "    \n",
    "else:\n",
    "    print(f\"the folder '{folder_path}' was not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117bf62",
   "metadata": {},
   "source": [
    "# **Main Pipeline** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40718274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 149 training images...\n",
      "Training Complete. Validation Accuracy: 100.00%\n",
      "ID0: 4230138:متسبلحى احمد سامح صلاح\n",
      "ID1: 1200277:مصطفى محمود احمد عويس\n",
      "ID10: 1220319:يوسف عمرو سيد كمال محمود محمد\n",
      "ID11: 1220092:يوسف محمد حمزه محمد عبدالتواب\n",
      "ID12: 1220188:ظ عمر محمد محمود صفوت احمد سعد الدين\n",
      "ID13: 1220062:ظ عبد الرحمن على السيد محمد السيد الحديدى\n",
      "ID14: 4230189:محمد تامر عبدالعزيز سالم\n",
      "ID15: 4230160:فادى سامى نبيل داود\n",
      "ID16: 4230142:ظ احمد مدحت عبدالعزيز على عوض اكت\n",
      "ID17: 1220279:مروان عمرو عبدالمجيد فؤاد احمد شكرى\n",
      "ID18: 1220256:عبدالله محمد جمال الدين احمد مصطفى\n",
      "ID19: 1220123:رغد وليد محمد نبيه فرج حسن\n",
      "ID2: 122017:ظ شاميه علاء محمد سيد احمد\n",
      "ID20: 1220137:لدين حمد عفيفي كريم محمد نجم الدين حمدي\n",
      "ID21: 4230159:عمرو ايهاب مختار فرحات\n",
      "ID22: 1220125:ا سارة محمد مصطفى جوده زهير\n",
      "ID23: 1200309:يوسف جلال محمد نور الدين جلال\n",
      "ID24: 4444444:هود جع وعم مرج رتح مو ونوج تج سح اموجن ادك كوي ييه كج وجوج د وا ججمن لجو اجاح لودج ترود اس روي جو ونيا و وجي الب واي لا سر جا وو رب سجر ام ا ال ا و لي الل الي جل دا ا ج و رو ت تحر واد كا ووو ا ن لط و جل يقح وي توق ب ماع ذا رد تززم حا ئاته جف اجتت لي أ رونا قل كل ل ا ا كن حر بمو فاك رعرة ون وي كد سي لوا دمر ل ل اي ا ا ا ل ل ا اخ ل ا اي ا كد د ا وو لات لمر ارا ا دروا ا بخ ب واي تر ل ل وب لي ل لج كا ا و لاجر ل ا جر ل ا ا ا را ل ا ا ا ا ا ا ا ا مرو ل رد لاو ابد د لا ولا دو ب وا ل رم ار ار و باد جل ل ل امت تق ند لحم جد ري لل عراز اح را ل ا و اللي و ار ف ا ا ل و لح ا وا رمن و رشان ووو تج د مق ار ا ع ام رد ل ا ل ف م وني ما و اموت رن ل د ل اه ا وب ا ا ل لوا و ا و د ل و و د ور لو و يك ا ل ا ل و ل و در لح ا وج ود ب الت ها ل اي ار رويد اك كد قور بل قد اف لل كار ل اللخ يا ل مارح لطر ل وت اي حامك بد وي جحي اا ا ا ا ا ا لا ا ا ل ا ا و رش ا ل و ا اي مت تنيت ردن نج شي د و يا أ ال ل رو رار رزو مرحي لوا جر و ولا لزني لل ار ا\n",
      "ID3: 1220165:اسمهان ابراهيم يوسف البيطار\n",
      "ID4: 1220145:في م هو ولدشري الة وه د ييبء\n",
      "ID5: 1220237:حسين محمد ماهر بهاءالدين\n",
      "ID6: 4230174:كمال محمد منير عبدالحميد\n",
      "ID7: 1220301:ا يوسف محمد حمدى محمد عثمان\n",
      "ID8: 1220205:ظ معاد امام احمد امام احمد\n",
      "ID9: 1220149:ا كا\n"
     ]
    }
   ],
   "source": [
    "def main_pipeline():\n",
    "    base_dir = os.getcwd()  # Current working directory\n",
    "    path_to_dataset = os.path.join(base_dir, 'Raw_IDs')\n",
    "    refrence_image_path = os.path.join(base_dir, 'Raw_IDs', 'ID14.jpg')\n",
    "    SVMclassifier = train_SVM_robust()\n",
    "    results = {}\n",
    "\n",
    "    for i in os.listdir(path_to_dataset):\n",
    "        if not i.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            continue\n",
    "        img_path = os.path.join(path_to_dataset, i)\n",
    "        aligned_img = align_images_sift(img_path, refrence_image_path)\n",
    "        name_img, digit_imgs = extract_name_and_digits(aligned_img)\n",
    "        student_id = os.path.splitext(i)[0]\n",
    "        save_student_name(student_id, name_img)\n",
    "        save_split_digits(student_id, digit_imgs)\n",
    "        digit_preds = []\n",
    "        for digit_img in digit_imgs:\n",
    "            feat = extract_hog_features(digit_img)\n",
    "            pred = SVMclassifier.predict([feat])[0]\n",
    "            digit_preds.append(str(pred))\n",
    "        code_str = ''.join(digit_preds)\n",
    "        print(f\"{student_id}: {code_str}:{extractname(f'./extracted_names/{student_id}_name.jpg')}\")\n",
    "        results[student_id] = code_str\n",
    "\n",
    "# Call the pipeline\n",
    "main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
